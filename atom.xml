<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>HAMSTERSI</title>
  <subtitle>生性涼薄 擇木而修</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://hamstersi.github.io/"/>
  <updated>2017-03-02T14:31:18.000Z</updated>
  <id>http://hamstersi.github.io/</id>
  
  <author>
    <name>Hamster Si</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2014 - 09 New Brighton @ Christchurch.md</title>
    <link href="http://hamstersi.github.io/2017/03/2014-09-New-Brighton-@-Christchurch/"/>
    <id>http://hamstersi.github.io/2017/03/2014-09-New-Brighton-@-Christchurch/</id>
    <published>2017-03-02T13:56:55.000Z</published>
    <updated>2017-03-02T14:31:18.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width:100%"><img src="http://wx4.sinaimg.cn/large/672d88aagy1fd8uvir0alj23b41v0npd.jpg" alt="New Brighton @ Christchurch"></div></div><div class="group-picture-row"><div class="group-picture-column" style="width:33.333333333333336%"><img src="http://wx1.sinaimg.cn/mw690/672d88aagy1fd8uv0rhn8j23b41v0npd.jpg" alt="New Brighton @ Christchurch"></div><div class="group-picture-column" style="width:33.333333333333336%"><img src="http://wx1.sinaimg.cn/mw690/672d88aagy1fd8uvdjjyyj23b41v0e81.jpg" alt="New Brighton @ Christchurch"></div><div class="group-picture-column" style="width:33.333333333333336%"><img src="http://wx2.sinaimg.cn/mw690/672d88aagy1fd8uuw8bghj23b41v0kjl.jpg" alt="New Brighton @ Christchurch"></div></div></div></div>]]></content>
    
    <summary type="html">
    
      &lt;script src=&quot;/assets/js/APlayer.min.js&quot;&gt; &lt;/script&gt;&lt;div class=&quot;group-picture&quot;&gt;&lt;div class=&quot;group-picture-container&quot;&gt;&lt;div class=&quot;group-picture-
    
    </summary>
    
      <category term="Diary" scheme="http://hamstersi.github.io/categories/Diary/"/>
    
    
      <category term="Neverland" scheme="http://hamstersi.github.io/tags/Neverland/"/>
    
      <category term="Helloworld" scheme="http://hamstersi.github.io/tags/Helloworld/"/>
    
  </entry>
  
  <entry>
    <title>CPU Metrics Reference - General Exploration</title>
    <link href="http://hamstersi.github.io/2017/03/CPU-Metrics-Reference-General-Exploration/"/>
    <id>http://hamstersi.github.io/2017/03/CPU-Metrics-Reference-General-Exploration/</id>
    <published>2017-03-02T12:29:19.000Z</published>
    <updated>2017-03-02T13:39:28.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p>To make your applications take advantage of CPU microarchitectures, you need to know how the application is utilizing available hardware resources. One way to obtain this knowledge is by using on-chip Performance Monitoring Units (PMUs). PMUs are dedicated pieces of logic within a CPU core that count specific hardware events as they occur on the system. Examples of these events may be Cache Misses or Branch Mispredictions. These events can be observed and combined to create useful high-level metrics such as Cycles per Instruction (CPI).</p><a id="more"></a><p><img src="http://wx1.sinaimg.cn/mw690/672d88aagy1fd8s0n9jsqj219r0c4n00.jpg" alt="Pipeline"></p><p>The pipeline of a modern high-performance CPU is quite complex. In the simplified view blow, the pipeline is divided conceptually into two halves, the Front-end and the Back-end. The Front-end is responsible for fetching the program code represented in architectural instructions and decoding them into one or more low-level hardware operations called <em>micro-ops</em> (uOps). The uOps are then fed to the Back-end in a process called <em>allocation</em>. Once allocated, the Back-end is responsible for monitoring when uOp’s data operands are available and executing the uOp in an available execution unit. The completion of a uOp’s execution is called <em>retirement</em>, and is where results of the uOp are committed to the architectural state (CPU registers or written back to memory). Usually, most uOps pass completely through the pipeline and retire, but sometimes speculatively fetched uOps may get cancelled before retirement – like in the case of mispredicted branches.<br><img src="http://wx4.sinaimg.cn/large/672d88aagy1fd8sap5wrqg20xc0h2jsp.gif" alt="uOp Categories"></p><h1 id="Front-End-Bound"><a href="#Front-End-Bound" class="headerlink" title="Front-End Bound"></a>Front-End Bound</h1><p>Superscalar processors can be conceptually divided into the ‘front-end’, where instructions are fetched and decoded into the operations that constitute them; and the ‘back-end’, where the required computation is performed. Each cycle, the front-end generates up to four of these operations placed into pipeline slots that then move through the back-end. Thus, for a given execution duration in clock cycles, it is easy to determine the maximum number of pipeline slots containing useful work that can be retired in that duration. The actual number of retired pipeline slots containing useful work, though, rarely equals this maximum. This can be due to several factors: some pipeline slots cannot be filled with useful work, either because the front-end could not fetch or decode instructions in time (‘Front-end bound’ execution) or because the back-end was not prepared to accept more operations of a certain kind (‘Back-end bound’ execution). Moreover, even pipeline slots that do contain useful work may not retire due to bad speculation. Front-end bound execution may be due to a large code working set, poor code layout, or microcode assists. Back-end bound execution may be due to long-latency operations or other contention for execution resources. Bad speculation is most frequently due to branch misprediction.</p><p>A significant proportion of pipeline slots are remaining empty. Possible reasons include a large code working size, poor code layout (requiring too many memory accesses per cycle to get sufficient instructions to fill four pipeline slots), or microcode assists.</p><h2 id="gt-Front-End-Latency"><a href="#gt-Front-End-Latency" class="headerlink" title="&gt; Front-End Latency"></a>&gt; Front-End Latency</h2><p>This metric represents a fraction of slots during which CPU was stalled due to front-end latency issues, such as instruction-cache misses, ITLB misses or fetch stalls after a branch misprediction. In such cases, the front-end delivers no uOps.</p><h3 id="gt-gt-ICache-Misses"><a href="#gt-gt-ICache-Misses" class="headerlink" title="&gt;&gt; ICache Misses"></a>&gt;&gt; ICache Misses</h3><p>To introduce new uOps into the pipeline, the core must either fetch them from a decoded instruction cache, or fetch the instructions themselves from memory and then decode them. In the latter path, the requests to memory first go through the L1I (level 1 instruction) cache that caches the recent code working set. Front-end stalls can accrue when fetched instructions are not present in the L1I. Possible reasons are a large code working set or fragmentation between hot and cold code. In the latter case, when a hot instruction is fetched into the L1I, any cold code on its cache line is brought along with it. This may result in the eviction of other, hotter code.</p><h3 id="gt-gt-ITLB-Overhead"><a href="#gt-gt-ITLB-Overhead" class="headerlink" title="&gt;&gt; ITLB Overhead"></a>&gt;&gt; ITLB Overhead</h3><p>In x86 architectures, mappings between virtual and physical memory are facilitated by a page table, which is kept in memory. To minimize references to this table, recently-used portions of the page table are cached in a hierarchy of ‘translation look-aside buffers’, or TLBs, which are consulted on every virtual address translation. As with data caches, the farther a request has to go to be satisfied, the worse the performance impact. This metric estimates the performance penalty of page walks induced on ITLB (instruction TLB) misses.</p><h3 id="gt-gt-Branch-Resteers"><a href="#gt-gt-Branch-Resteers" class="headerlink" title="&gt;&gt; Branch Resteers"></a>&gt;&gt; Branch Resteers</h3><p>This metric represents cycles fraction the CPU was stalled due to Branch Resteers. Branch Resteers estimates the Frontend delay in fetching operations from corrected path, following all sorts of mispredicted branches. For example, branchy code with lots of misprediction might get categorized under Branch Resteers. Note the value of this node may overlap with its siblings.</p><h3 id="gt-gt-DSB-Switches"><a href="#gt-gt-DSB-Switches" class="headerlink" title="&gt;&gt; DSB Switches"></a>&gt;&gt; DSB Switches</h3><p>Intel microarchitecture code name Sandy Bridge introduces a new decoded ICache. This cache, called the DSB (Decoded Stream Buffer), stores uOps that have already been decoded, avoiding many of the penalties of the legacy decode pipeline, called the MITE (Micro-instruction Translation Engine). However, when control flows out of the region cached in the DSB, the front-end incurs a penalty as uOp issue switches from the DSB to the MITE. This metric measures this penalty.</p><h3 id="gt-gt-Length-Changing-Prefixes"><a href="#gt-gt-Length-Changing-Prefixes" class="headerlink" title="&gt;&gt; Length Changing Prefixes"></a>&gt;&gt; Length Changing Prefixes</h3><p>This metric represents a fraction of cycles during which CPU was stalled due to Length Changing Prefixes (LCPs). To avoid this issue, use proper compiler flags. Intel Compiler enables these flags by default.</p><h3 id="gt-gt-MS-Switches"><a href="#gt-gt-MS-Switches" class="headerlink" title="&gt;&gt; MS Switches"></a>&gt;&gt; MS Switches</h3><p>This metric represents a fraction of cycles when the CPU was stalled due to switches of uOp delivery to the Microcode Sequencer (MS). Commonly used instructions are optimized for delivery by the DSB or MITE pipelines. Certain operations cannot be handled natively by the execution pipeline, and must be performed by microcode (small programs injected into the execution stream). Switching to the MS too often can negatively impact performance. The MS is designated to deliver long uOp flows required by CISC instructions like CPUID, or uncommon conditions like Floating Point Assists when dealing with Denormals.</p><h2 id="gt-Front-End-Bandwidth"><a href="#gt-Front-End-Bandwidth" class="headerlink" title="&gt; Front-End Bandwidth"></a>&gt; Front-End Bandwidth</h2><p>This metric represents a fraction of slots during which CPU was stalled due to front-end bandwidth issues, such as inefficiencies in the instruction decoders or code restrictions for caching in the DSB (decoded uOps cache). In such cases, the front-end typically delivers a non-optimal amount of uOps to the back-end.</p><h3 id="gt-gt-Front-End-Bandwidth-MITE"><a href="#gt-gt-Front-End-Bandwidth-MITE" class="headerlink" title="&gt;&gt; Front-End Bandwidth MITE"></a>&gt;&gt; Front-End Bandwidth MITE</h3><p>This metric represents a fraction of cycles during which CPU was stalled due to the MITE fetch pipeline issues, such as inefficiencies in the instruction decoders.</p><h3 id="gt-gt-Front-End-Bandwidth-DSB"><a href="#gt-gt-Front-End-Bandwidth-DSB" class="headerlink" title="&gt;&gt; Front-End Bandwidth DSB"></a>&gt;&gt; Front-End Bandwidth DSB</h3><p>This metric represents a fraction of cycles during which CPU was likely limited due to DSB (decoded uOp cache) fetch pipeline. For example, inefficient utilization of the DSB cache structure or bank conflict when reading from it, are categorized here.</p><h3 id="gt-gt-Front-End-Bandwidth-LSD"><a href="#gt-gt-Front-End-Bandwidth-LSD" class="headerlink" title="&gt;&gt; Front-End Bandwidth LSD"></a>&gt;&gt; Front-End Bandwidth LSD</h3><p>This metric represents a fraction of cycles during which CPU operation was limited by the LSD (Loop Stream Detector) unit. Typically, LSD provides good uOp supply. However, in some rare cases, optimal uOp delivery cannot be reached for small loops whose size (in terms of number of uOps) does not suit well the LSD structure.</p><h1 id="Bad-Speculation"><a href="#Bad-Speculation" class="headerlink" title="Bad Speculation"></a>Bad Speculation</h1><h2 id="gt-Branch-Mispredict"><a href="#gt-Branch-Mispredict" class="headerlink" title="&gt; Branch Mispredict"></a>&gt; Branch Mispredict</h2><p>When a branch mispredicts, some instructions from the mispredicted path still move through the pipeline. All work performed on these instructions is wasted since they would not have been executed had the branch been correctly predicted. This metric represents slots fraction the CPU has wasted due to Branch Misprediction. These slots are either wasted by uOps fetched from an incorrectly speculated program path, or stalls when the out-of-order part of the machine needs to recover its state from a speculative path.</p><h2 id="gt-Machine-Clears"><a href="#gt-Machine-Clears" class="headerlink" title="&gt; Machine Clears"></a>&gt; Machine Clears</h2><p>Certain events require the entire pipeline to be cleared and restarted from just after the last retired instruction. This metric measures three such events: memory ordering violations, self-modifying code, and certain loads to illegal address ranges. Machine Clears metric represents slots fraction the CPU has wasted due to Machine Clears. These slots are either wasted by uOps fetched prior to the clear, or stalls the out-of-order portion of the machine needs to recover its state after the clear.</p><h1 id="Back-End-Bound"><a href="#Back-End-Bound" class="headerlink" title="Back-End Bound"></a>Back-End Bound</h1><p>Identify slots where no uOps are delivered due to a lack of required resources for accepting more uOps in the back-end of the pipeline. Back-end metrics describe a portion of the pipeline where the out-of-order scheduler dispatches ready uOps into their respective execution units, and, once completed, these uOps get retired according to program order. Stalls due to data-cache misses or stalls due to the overloaded divider unit are examples of back-end bound issues.</p><h2 id="gt-Memory-Bound"><a href="#gt-Memory-Bound" class="headerlink" title="&gt; Memory Bound"></a>&gt; Memory Bound</h2><p>This metric shows how memory subsystem issues affect the performance. Memory Bound measures a fraction of slots where pipeline could be stalled due to demand load or store instructions. This accounts mainly for incomplete in-flight memory demand loads that coincide with execution starvation in addition to less common cases where stores could imply back-pressure on the pipeline.</p><h3 id="gt-gt-L1-Bound"><a href="#gt-gt-L1-Bound" class="headerlink" title="&gt;&gt; L1 Bound"></a>&gt;&gt; L1 Bound</h3><p>This metric shows how often machine was stalled without missing the L1 data cache. The L1 cache typically has the shortest latency. However, in certain cases like loads blocked on older stores, a load might suffer a high latency even though it is being satisfied by the L1.</p><h4 id="gt-gt-gt-DTLB-Overhead"><a href="#gt-gt-gt-DTLB-Overhead" class="headerlink" title="&gt;&gt;&gt; DTLB Overhead"></a>&gt;&gt;&gt; DTLB Overhead</h4><p>This metric estimates the performance penalty paid for missing the first-level data TLB (DTLB) that includes hitting in the second-level data TLB (STLB) as well as performing a hardware page walk on an STLB miss.</p><h4 id="gt-gt-gt-Loads-Blocked-by-Store-Forwarding"><a href="#gt-gt-gt-Loads-Blocked-by-Store-Forwarding" class="headerlink" title="&gt;&gt;&gt; Loads Blocked by Store Forwarding"></a>&gt;&gt;&gt; Loads Blocked by Store Forwarding</h4><p>To streamline memory operations in the pipeline, a load can avoid waiting for memory if a prior store, still in flight, is writing the data that the load wants to read (a ‘store forwarding’ process). However, in some cases, generally when the prior store is writing a smaller region than the load is reading, the load is blocked for a significant time pending the store forward. This metric measures the performance penalty of such blocked loads.</p><h4 id="gt-gt-gt-Lock-Latency"><a href="#gt-gt-gt-Lock-Latency" class="headerlink" title="&gt;&gt;&gt; Lock Latency"></a>&gt;&gt;&gt; Lock Latency</h4><p>This metric represents cycles fraction the CPU spent handling cache misses due to lock operations. Due to the microarchitecture handling of locks, they are classified as L1 Bound regardless of what memory source satisfied them.</p><h4 id="gt-gt-gt-Split-Loads"><a href="#gt-gt-gt-Split-Loads" class="headerlink" title="&gt;&gt;&gt; Split Loads"></a>&gt;&gt;&gt; Split Loads</h4><p>Throughout the memory hierarchy, data moves at cache line granularity - 64 bytes per line. Although this is much larger than many common data types, such as integer, float, or double, unaligned values of these or other types may span two cache lines. Recent Intel architectures have significantly improved the performance of such ‘split loads’ by introducing split registers to handle these cases, but split loads can still be problematic, especially if many split loads in a row consume all available split registers.</p><h4 id="gt-gt-gt-4K-Aliasing"><a href="#gt-gt-gt-4K-Aliasing" class="headerlink" title="&gt;&gt;&gt; 4K Aliasing"></a>&gt;&gt;&gt; 4K Aliasing</h4><p>When an earlier (in program order) load issued after a later (in program order) store, a potential WAR (write-after-read) hazard exists. To detect such hazards, the memory order buffer (MOB) compares the low-order 12 bits of the load and store in every potential WAR hazard. If they match, the load is reissued, penalizing performance. However, as only 12 bits are compared, a WAR hazard may be detected falsely on loads and stores whose addresses are separated by a multiple of 4096 (2^12). This metric estimates the performance penalty of handling such falsely aliasing loads and stores.</p><h4 id="gt-gt-gt-FB-Full"><a href="#gt-gt-gt-FB-Full" class="headerlink" title="&gt;&gt;&gt; FB Full"></a>&gt;&gt;&gt; FB Full</h4><p>This metric does a rough estimation of how often L1D Fill Buffer unavailability limited additional L1D miss memory access requests to proceed. The higher the metric value, the deeper the memory hierarchy level the misses are satisfied from. Often it hints on approaching bandwidth limits (to L2 cache, L3 cache or external memory).</p><h3 id="gt-gt-L2-Bound"><a href="#gt-gt-L2-Bound" class="headerlink" title="&gt;&gt; L2 Bound"></a>&gt;&gt; L2 Bound</h3><p>This metric shows how often machine was stalled on L2 cache. Avoiding cache misses (L1 misses/L2 hits) will improve the latency and increase performance.</p><h3 id="gt-gt-L3-Bound"><a href="#gt-gt-L3-Bound" class="headerlink" title="&gt;&gt; L3 Bound"></a>&gt;&gt; L3 Bound</h3><p>This metric shows how often CPU was stalled on L3 cache, or contended with a sibling Core. Avoiding cache misses (L2 misses/L3 hits) improves the latency and increases performance.</p><h4 id="gt-gt-gt-Contested-Accesses"><a href="#gt-gt-gt-Contested-Accesses" class="headerlink" title="&gt;&gt;&gt; Contested Accesses"></a>&gt;&gt;&gt; Contested Accesses</h4><p>Contested accesses occur when data written by one thread is read by another thread on a different core. Examples of contested accesses include synchronizations such as locks, true data sharing such as modified locked variables, and false sharing. This metric is a ratio of cycles generated while the caching system was handling contested accesses to all cycles.</p><h4 id="gt-gt-gt-Data-Sharing"><a href="#gt-gt-gt-Data-Sharing" class="headerlink" title="&gt;&gt;&gt; Data Sharing"></a>&gt;&gt;&gt; Data Sharing</h4><p>Data shared by multiple threads (even just read shared) may cause increased access latency due to cache coherency. This metric measures the impact of that coherency. Excessive data sharing can drastically harm multithreaded performance. This metric is defined by the ratio of cycles while the caching system is handling shared data to all cycles. It does not measure waits due to contention on a variable, which is measured by the Locks and Waits analysis.</p><h4 id="gt-gt-gt-L3-Latency"><a href="#gt-gt-gt-L3-Latency" class="headerlink" title="&gt;&gt;&gt; L3 Latency"></a>&gt;&gt;&gt; L3 Latency</h4><p>This metric shows a fraction of cycles with demand load accesses that hit the L3 cache under unloaded scenarios (possibly L3 latency limited). Avoiding private cache misses (i.e. L2 misses/L3 hits) will improve the latency, reduce contention with sibling physical cores and increase performance. Note the value of this node may overlap with its siblings.</p><h4 id="gt-gt-gt-SQ-Full"><a href="#gt-gt-gt-SQ-Full" class="headerlink" title="&gt;&gt;&gt; SQ Full"></a>&gt;&gt;&gt; SQ Full</h4><p>This metric measures fraction of cycles where the Super Queue (SQ) was full taking into account all request-types and both hardware SMT threads. The Super Queue is used for requests to access the L2 cache or to go out to the Uncore.</p><h3 id="gt-gt-DRAM-Bound"><a href="#gt-gt-DRAM-Bound" class="headerlink" title="&gt;&gt; DRAM Bound"></a>&gt;&gt; DRAM Bound</h3><p>This metric shows how often CPU was stalled on the main memory (DRAM). Caching typically improves the latency and increases performance.</p><h4 id="gt-gt-gt-Memory-Bandwidth"><a href="#gt-gt-gt-Memory-Bandwidth" class="headerlink" title="&gt;&gt;&gt; Memory Bandwidth"></a>&gt;&gt;&gt; Memory Bandwidth</h4><p>This metric represents a fraction of cycles during which an application could be stalled due to approaching bandwidth limits of the main memory (DRAM). This metric does not aggregate requests from other threads/cores/sockets (see Uncore counters for that). Consider improving data locality in NUMA multi-socket systems.</p><h4 id="gt-gt-gt-Memory-Latency"><a href="#gt-gt-gt-Memory-Latency" class="headerlink" title="&gt;&gt;&gt; Memory Latency"></a>&gt;&gt;&gt; Memory Latency</h4><p>This metric represents a fraction of cycles during which an application could be stalled due to the latency of the main memory (DRAM). This metric does not aggregate requests from other threads/cores/sockets (see Uncore counters for that). Consider optimizing data layout or using Software Prefetches (through the compiler).</p><p><strong>LLC Miss: </strong>The LLC (last-level cache) is the last, and longest-latency, level in the memory hierarchy before main memory (DRAM). Any memory requests missing here must be serviced by local or remote DRAM, with significant latency. The LLC Miss metric shows a ratio of cycles with outstanding LLC misses to all cycles.</p><h3 id="gt-gt-Store-Bound"><a href="#gt-gt-Store-Bound" class="headerlink" title="&gt;&gt; Store Bound"></a>&gt;&gt; Store Bound</h3><p>This metric shows how often CPU was stalled on store operations. Even though memory store accesses do not typically stall out-of-order CPUs; there are few cases where stores can lead to actual stalls. Consider False Sharing analysis as your next step.</p><h4 id="gt-gt-gt-Store-Latency"><a href="#gt-gt-gt-Store-Latency" class="headerlink" title="&gt;&gt;&gt; Store Latency"></a>&gt;&gt;&gt; Store Latency</h4><p>This metric represents cycles fraction the CPU spent handling long-latency store misses (missing 2nd level cache). Consider avoiding/reducing unnecessary (or easily loadable/computable) memory store. Note that this metric value may be highlighted due to a Lock Latency issue.</p><h4 id="gt-gt-gt-False-Sharing"><a href="#gt-gt-gt-False-Sharing" class="headerlink" title="&gt;&gt;&gt; False Sharing"></a>&gt;&gt;&gt; False Sharing</h4><p>This metric shows how often CPU was stalled on store operations to a shared cache line. It can be easily avoided by padding to make threads access different lines.</p><h4 id="gt-gt-gt-Split-Stores"><a href="#gt-gt-gt-Split-Stores" class="headerlink" title="&gt;&gt;&gt; Split Stores"></a>&gt;&gt;&gt; Split Stores</h4><p>This metric represents a rate of split store accesses. Consider aligning your data to the 64-byte cache line granularity.</p><h4 id="gt-gt-gt-DTLB-Store-Overhead"><a href="#gt-gt-gt-DTLB-Store-Overhead" class="headerlink" title="&gt;&gt;&gt; DTLB Store Overhead"></a>&gt;&gt;&gt; DTLB Store Overhead</h4><p>This metric represents a fraction of cycles spent on handling first-level data TLB store misses. As with ordinary data caching, focus on improving data locality and reducing working-set size to reduce DTLB overhead. Additionally, consider using profile-guided optimization (PGO) to collocate frequently-used data on the same page. Try using larger page sizes for large amounts of frequently-used data.</p><h2 id="gt-Core-Bound"><a href="#gt-Core-Bound" class="headerlink" title="&gt; Core Bound"></a>&gt; Core Bound</h2><p>This metric represents how much Core non-memory issues were of a bottleneck. Shortage in hardware compute resources, or dependencies software’s instructions are both categorized under Core Bound. Hence it may indicate the machine ran out of an OOO resources, certain execution units are overloaded or dependencies in program’s data- or instruction- flow are limiting the performance (e.g. FP-chained long-latency arithmetic operations).</p><h3 id="gt-gt-Divider"><a href="#gt-gt-Divider" class="headerlink" title="&gt;&gt; Divider"></a>&gt;&gt; Divider</h3><p>Not all arithmetic operations take the same amount of time. Divides and square roots, both performed by the DIV unit, take considerably longer than integer or floating point addition, subtraction, or multiplication. This metric represents cycles fraction where the Divider unit was active.</p><h3 id="gt-gt-Port-Utilization"><a href="#gt-gt-Port-Utilization" class="headerlink" title="&gt;&gt; Port Utilization"></a>&gt;&gt; Port Utilization</h3><p>This metric represents a fraction of cycles during which an application was stalled due to Core non-divider-related issues. For example, heavy data-dependency between nearby instructions, or a sequence of instructions that overloads specific ports. Hint: Loop Vectorization - most compilers feature auto-Vectorization options today - reduces pressure on the execution ports as multiple elements are calculated with same uOp.</p><h1 id="Retiring"><a href="#Retiring" class="headerlink" title="Retiring"></a>Retiring</h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;To make your applications take advantage of CPU microarchitectures, you need to know how the application is utilizing available hardware resources. One way to obtain this knowledge is by using on-chip Performance Monitoring Units (PMUs). PMUs are dedicated pieces of logic within a CPU core that count specific hardware events as they occur on the system. Examples of these events may be Cache Misses or Branch Mispredictions. These events can be observed and combined to create useful high-level metrics such as Cycles per Instruction (CPI).&lt;/p&gt;
    
    </summary>
    
      <category term="Tech" scheme="http://hamstersi.github.io/categories/Tech/"/>
    
    
      <category term="Architecture" scheme="http://hamstersi.github.io/tags/Architecture/"/>
    
      <category term="CPU" scheme="http://hamstersi.github.io/tags/CPU/"/>
    
  </entry>
  
  <entry>
    <title>Economics in One Lesson</title>
    <link href="http://hamstersi.github.io/2017/02/Economics-in-One-Lesson/"/>
    <id>http://hamstersi.github.io/2017/02/Economics-in-One-Lesson/</id>
    <published>2017-02-19T08:29:47.000Z</published>
    <updated>2017-03-02T13:45:04.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h3 id="关于这堂课"><a href="#关于这堂课" class="headerlink" title="关于这堂课"></a>关于这堂课</h3><p>经济学的艺术，在于不仅要观察任何行为或整车的即期影响，更要考察比较长远的影响；不仅要关注政策对某个群体产生的影响，更要追踪对所有群体造成的影响。</p><h3 id="课程的应用"><a href="#课程的应用" class="headerlink" title="课程的应用"></a>课程的应用</h3><h4 id="gt-破橱窗"><a href="#gt-破橱窗" class="headerlink" title="&gt; 破橱窗"></a>&gt; 破橱窗</h4><p>话说一个顽童抡起砖头，砸破了面包店的橱窗。当店主怒气冲冲追出来时，小捣蛋早已逃之夭夭，只剩下一群看闹热的围观者。大家盯着橱窗的破洞以及四下散落的玻璃碎片，有些人开始互相议论，宽慰店主：玻璃破了很是可惜，可是这也有好的一面。这不，对面的玻璃店又有生意了。他们越琢磨越来劲：一面新的橱窗需要250美元。玻璃店多了250美元，会去别的商家那里消费，那些个商家的口袋里多了几个钱，又会向更多的商家买东西。经这么一说，小小一片破橱窗，竟能够连环不断提供资金给很多商家，使很多人获得就业机会。要是照这个逻辑推下去，结论便是：扔砖头的那个小捣蛋，不但不是社区的祸害，反而是造福社区的善人。</p><a id="more"></a><p>但是，面包店主损失掉的250美元，原本是打算拿去做一套西装的。如今，这钱被迫挪去补破窗。他原来在准备去做西装的那个下午，他本来可以心满意足同时拥有橱窗和西装，结果却只能面对有了橱窗就没了西装的糟糕现实。简单来说，玻璃店主的这桩生意，不过是从做西装的缝纫店主那里转移来的。整个过程并没有新增“就业机会”。那些围观的人只想到了交易双方：面包店主和玻璃店主的情况，却忘掉了可能涉及的第三方缝纫店主的窘迫。</p><h4 id="gt-战祸之福"><a href="#gt-战祸之福" class="headerlink" title="&gt; 战祸之福"></a>&gt; 战祸之福</h4><p>有些人不屑于谈小小的破坏行为带来的蝇头小利，却醉心于巨大的破坏行为能让人们受益无穷。他们吹嘘战争对经济是如何如何的有利，非和平时期能比，并向我们展示通过战争才能实现的“生产奇迹”。他们认为，战争时期庞大的需求“累积”或“堵塞”，会给战后的世界带来繁荣。</p><p>这种“需求堵塞”谬论只不过是我们所熟悉的老朋友——破窗谬论——换上一件臃肿的马甲之后的形象而已。不过这一次，有更多相关的谬误绞缠在一起，需要我们逐一驳斥。首先，它把需要(need)和需求(demand)混为一谈。战火摧毁的东西越多，人民生活越贫困，战后需要的东西也就越多，这点毋庸置疑。但是，需要并不等于需求。有效的经济需求，光有需要还不算，还必须要有相当的购买力才行。当今印度对产品的实际需要远高于美国，但是它的购买力，以及由此带来的创造商机的能力却远低于美国。</p><p>人们只习惯于用金钱来衡量自己的财富和收入，所以只要手头多了几张钞票，便以为自己过得更好，尽管拿这些钱能买到的东西比从前少，自己实际拥有的东西可能不如从前。人们所认为的第二次世界大战带来的经济“收益”，其实大多是战时通货膨胀造成的幻象。哪怕在和平年代，同等规模的通货膨胀也能带来这样的结果，并且的确产生过这些结果。</p><p>那些认为战争造成的破坏能增加总体“需求”的人，还遗漏了一个基本事实：需求和供给就像硬币的两面，其实是从不同角度观察到的同一样东西。供给会创造需求，因为归根结底供给就是需求。人们把自己生产的东西供应给他人，其实是为了换得自己想要的东西。农民为城市供应小麦，是因为他们需要汽车或其他产品。所有这些，是现代分工和交换经济的本质。</p><h4 id="gt-公共工程来自缴税"><a href="#gt-公共工程来自缴税" class="headerlink" title="&gt; 公共工程来自缴税"></a>&gt; 公共工程来自缴税</h4><p>我们必须认识到：政府所有的支出最后都必须靠纳税人来埋单；通货膨胀本身只是税收的一种表现形式，并且是极其有害的形式。</p><p>一定数额的公共开支对执行基本的政府职能是必要的。一定数量的公共设施建设，如街道、桥梁、隧道、军营、海军基地、议会大厦、警察局和消防队，是提供基本的公共服务所必需的。社会对这些公共建设本身有需要，需要就是其存在的理由。但如果是为了“提高就业机会”而建桥，那就成另外一回事了。当提供就业机会成了目的之后，有无兴建桥梁的实际需要就会成为次要问题。这时，政府必须无中生有，发明各种“公共建设计划”。他们不再只考虑哪里必须建桥，而是开始自问自答：桥可以建在哪里。</p><p>不难发现，造桥工程不过是使就业机会发生了转移而已。造桥工程每制造一个用于公共建设上的工作机会，就必定会破坏掉一个用于私人领域的工作机会。我们看得见桥梁工地上建设者们夜以继日，但有些东西我们是看不到的。这些看不到的东西，是从纳税人的口袋拿走1000万美元之后而破坏掉的工作机会，一方面造桥工人在增加，另一方面汽车工人、电视机工人、制衣工人、农民在越少。</p><h4 id="gt-税负抑制生产"><a href="#gt-税负抑制生产" class="headerlink" title="&gt; 税负抑制生产"></a>&gt; 税负抑制生产</h4><p>我们都知道，在现代社会中，每个人所承受的所得税比例不尽相同。为了弥补公共开支，政府还必须开征其他名目繁多的赋税，从而进一步挫伤了生产者的积极性。没有人情愿自己的钱被政府拿走，税负不可避免地会影响纳税人的行为和动机。如果一家公司发生亏损，每赔一块钱，就得足足损失一块钱；要是没有适当的税务会计法规，允许拿数年来的亏损去冲抵数年来的利润的话，当这家公司赚钱的时候，每赚一块钱，却只能留下税后的部分(例如52美分)。于是，公司的经营政策就会受到影响，它将丧失扩张业务的冲动，或者只扩张那些风险最低的业务。觉察到这种状况的人甚至会打消开创新事业的念头。现有的雇主将不再增加员工，甚至会设法减员；其他人则根本不打算成为雇主。长期下来，与本来可以达到的水准相比，新设备和新工艺的应用放慢，消费者买不到更好更便宜的产品，实际工资不升反降。</p><h4 id="gt-政府信贷扭曲生产"><a href="#gt-政府信贷扭曲生产" class="headerlink" title="&gt; 政府信贷扭曲生产"></a>&gt; 政府信贷扭曲生产</h4><p>有时，政府拿钱出来“扶持”企业，这其实和政府征敛民财一样可怕。扶持的形式通常表现为政府直接贷款，以及政府担保的民间贷款等。</p><p>私人提供贷款与政府提供贷款这二者有根本的不同。每个私营放贷者都是在用自己的资金承担风险。银行家虽然是利用别人委托给他的钱去冒险，但一旦有损失，也必须拿自己的钱去赔付，否则只有破产出局。当人们拿自己的钱去冒险时，通常会严格审查借款人是否有足够的资产做抵押，审查其经营能力和诚信如何。</p><p>倘若政府依照同样严格的标准去办理贷款业务，那政府根本没有必要涉足这一行，去做私营机构已经在做的事。政府之所以涉足贷款业，就是要向那些从私营机构借不到钱的人提供资金。换句话说，私营放贷者不肯拿自己的钱去冒的风险，政府却愿意拿纳税人的钱去冒险。这种措施的支持者也承认政府放贷的坏帐率高于民间放贷，但他们坚持认为，那些有借有还的人所增加的产出，加上大部分有借无还的人所增加的产出，在抵消坏账损失后仍有剩余。</p><p>政府贷款的一种后果：浪费资本和削弱生产。政府信贷会把可用的资本丢进糟糕的计划中，充其量也是丢进好坏难断的计划中。政府信贷更可能把资本交给能力比较差、或者比较不可靠的人。可是在任何时候，实体资本的数量都是有限的(有别于靠印钞机印出来的货币资本)，交到某乙手中的东西，就不可能再交给某甲。</p><h4 id="gt-诅咒机器"><a href="#gt-诅咒机器" class="headerlink" title="&gt; 诅咒机器"></a>&gt; 诅咒机器</h4><p>在所有的经济错觉中，“机器必然导致失业”这种错误最为阴魂不散。这种错觉曾经被无数人驳倒过，但总能死灰复燃，并且和以往一样张狂。</p><p>如果采用省力机器确实会造成失业率不断上升、加剧不幸的话，我们将合乎逻辑地得出颠覆性的结论，不仅会颠覆技术领域的观念，而且会颠覆整个人类文明的观念。我们不仅应该把任何的新技术进步都视为一场灾难，而且更该觉得过去所有的技术进步也都同样恐怖。大大小小的雇主，总在设法通过节约劳动力来提高经济效益。头脑灵活的工人，都会想办法以最少的付出去完成上面指派的工作。雄心勃勃的人，总在坚持不懈地跟时间赛跑。如果严守逻辑上的一致性，那么恐惧科技进步的人必须摒弃所有这些进步和智巧，因为技术进步不但无益，而且有害。比方说从芝加哥运货到纽约，要是我们能够大量雇用人力，我们何必还要用火车，让人扛起货物背过去得了。</p><p>我们需要重申，机器所带来的是促进生产和提高生活水平。这个结果可以通过两条途径来实现：机器使消费者购买的产品变得更加便宜，或者提高工人的生产力，从而使工人的工资能够提高。换句话说，机器能够提高货币工资，或者能够降低物价，让同样的薪水能买到更多的产品和服务。</p><h4 id="gt-分散工作机会的企图"><a href="#gt-分散工作机会的企图" class="headerlink" title="&gt; 分散工作机会的企图"></a>&gt; 分散工作机会的企图</h4><p>“制造工作机会”和“限产超雇”的这些做法的起因以及公众容忍它们的起因，跟害怕机器的起因一样，都源自相同的基本谬论：用更有效率的方式去做事，只会消减工作机会。这个信条换句话说就是，采用低效率的方式去做一件事，反而可以创造工作机会。和这个谬论有关的另一种论调认为，世界上可做的工作是有限的，要是我们想不出更繁琐拖沓的做事方式来增加工作量，那么我们至少可以想方设法将事情分摊给尽可能多的人去做。</p><p>倘若大家各自为政，少数人的确可以从这种不合理的细部分工规定中获利，然而这种呆板的规定是以牺牲其他人的利益为代价的。支持这类做法的人没有看到生产成本一定会因此增高，从总体上看，最终结果是工作做得更少，生产出的产品更少。</p><h4 id="gt-遣散军队和裁减公务员"><a href="#gt-遣散军队和裁减公务员" class="headerlink" title="&gt; 遣散军队和裁减公务员"></a>&gt; 遣散军队和裁减公务员</h4><p>每逢战争结束，军人复员的时候，人们总是担忧没有足够多的工作岗位来安置这些复员军人，担心这些人会失业。但是，军人复员转业之后的经济，与遣散之前的经济不会相同。以前靠平民养活的军人，而今是自食其力的平民。战争结束了，国防建设也用不了那么多军人，继续把他们留在军中就是徒劳无益的事情，因为纳税人出钱供养他们是得不到相应回报的。现在，纳税人能把这一部分的钱支付给复员转业人员，换取等值的产品或服务。国民生产总值，以及每个人的财富，都会增加更多。同样的推理也适用于政府裁减冗员的情况。由于人浮于事，这些公务员为社会提供的服务配不上他们所获得的薪酬。</p><p>裁掉冗员之后的国家比留用冗员时，不会更糟，只会更好。因为这部分人必须自谋职业或自己创业。纳税人购买力增加，将为被裁公务员提供更多就业机会，这跟军人复员的情形一样。并且，这些人只有为雇主或顾客提供等值服务，才能在社会立足。他们不再是社会的寄生虫，而成为直接为社会贡献生产力的个人。</p><h4 id="gt-盲目崇拜充分就业"><a href="#gt-盲目崇拜充分就业" class="headerlink" title="&gt; 盲目崇拜充分就业"></a>&gt; 盲目崇拜充分就业</h4><p>与每个人的经济目标一样，任何国家的经济目标也应该是用最少的付出获得最大的收益。中国和印度远比美国贫穷，但它们的主要问题也不是失业问题，而是生产方式太落后(这既是资本短缺的原因，也是资本短缺的结果)。</p><p>文明的进步其实体现在就业人口的减少上，而不是体现在增加上。这是因为当国家变得日益富裕之后，我们得以禁止使用童工，得以实现老有所养。只要我们把重点放在该放的地方，也就是采取生产最大化的政策，我们就不会迷失。</p><h4 id="gt-关税“保护”了哪些人？"><a href="#gt-关税“保护”了哪些人？" class="headerlink" title="&gt; 关税“保护”了哪些人？"></a>&gt; 关税“保护”了哪些人？</h4><p>关税常被看作是以牺牲消费者来造福生产者的一种手段。从某种意义上讲，这话大致不错。那些主张征收关税的人，只考虑到受关税保护的生产者能立即获得利益，却忽略被迫支付关税的消费者会立即遭受损失。但把关税只看成是生产者利益与消费者利益之间的冲突，却是不对的。关税的确让所有的消费者蒙受损失，但并非所有的生产者都从中获益。前面谈过，关税只对受保护的生产者有帮助，为此不惜牺牲其他所有的美国生产者：尤其是对那些出口潜力相对较大的生产者利益的损害。</p><h4 id="gt-拼命出口"><a href="#gt-拼命出口" class="headerlink" title="&gt; 拼命出口"></a>&gt; 拼命出口</h4><p>长期而言，进口与出口必然相等(这里的进出口指“国际收支账户”里的所有项目，包括“无形”项目，如旅游消费、海运费用等)。有出口，才有钱进口；没有进口，就没有机会出口，因为外国人没有美元可以用来买美国的产品。要想扩大出口，就必须有更多的进口，否则收不到货款。要想压缩进口，出口也会随之被压缩。</p><h4 id="gt-“平准价格”"><a href="#gt-“平准价格”" class="headerlink" title="&gt; “平准价格”"></a>&gt; “平准价格”</h4><p>平准价格的说法大致如下。在所有产业中，农业最基本、最重要，必须不惜一切代价加以保护。还说，只有农民富裕了，其他人的富裕才有着落。要是农民缺乏购买力，买不起工业产品，工商业就会萎缩。</p><p>政府干预并没有停留在造成购买力的转移，也就是把购买力从城市消费者(或一般纳税人，或两者兼而有之)，转移到农民手上。政府干预往往通过限制农产品的生产以抬高价格，这样会造成财富灭失，因为可供消费农产品被人为减少了。</p><h4 id="gt-救救X产业"><a href="#gt-救救X产业" class="headerlink" title="&gt; 救救X产业"></a>&gt; 救救X产业</h4><p>拯救X产业的办法不胜枚举。相关提案大抵有两大类。其中一类强调，X产业已经“过度拥挤”，政府应该阻止其他公司或劳工再进入。另一类则呼吁，政府应该以直接补贴的方式对X产业给予扶持。</p><p>如果X产业与其他产业相比确实过度拥挤，那根本不需要任何强制立法，去排斥新资本或新劳工进入。新资本不会抢着挤进显然要垮掉的行业。投资人不会冒然踏进风险最大、回报率最低的行业。劳工如果能更好的选择，同样不会进入工资最低、工作最不稳定的行业。如果新资本和新劳工是被强制排斥在X产业之外，例如垄断、同业联盟、工会的政策或者法律等强制手段剥夺了这些资源的自由选择。投资人只好将钱投向回报率还不如X产业的其他地方。其结果是产量减少，进而导致生活水平普遍下降。</p><p>从国库直接拿钱来补贴X产业，这只不过是将财富或收益转移到X产业。X产业中的人获得多少，纳税人就损失多少。但是，补贴的结果不仅仅是财富或收益的转移，也不仅仅意味其他产业的总体萎缩程度和X产业的扩张程度相当。</p><h4 id="gt-价格体系如何运作？"><a href="#gt-价格体系如何运作？" class="headerlink" title="&gt; 价格体系如何运作？"></a>&gt; 价格体系如何运作？</h4><p>在现代这样一个错综复杂的社会中，人们的需要和要求及其轻重缓急千差万别，如何解决劳力和资本用在哪里的问题？解决之道正是通过价格体系，也就是通过生产成本、价格和利润三者间不断变动的交互关系来调节的。</p><p>生产成本决定价格这样的理论是不对的。价格取决于供给和需求，而需求取决于人们想要拥有某种商品渴求程度，以及由人们用来交换的东西所决定。认为供给有一部分取决于生产成本是对的。但一种商品已经发生的生产成本却不能决定商品的价值。商品价值取决于现在发生的供需关系。对于一种商品未来生产成本和市场价位的预期，将会决定<br>那种商品未来的生产数量，这种预期就会影响未来的供给。因此，一种商品的价格与其边际生产成本总是趋于彼此相等，但并不是由于边际生产成本直接决定价格。</p><p>经济处于均衡状态时，某个行业的扩张，必然要以其他行业牺牲为代价。生产要素在任何时候都是有限的。只有当劳工、土地和资本，从原来的行业转移到了某个行业，那个行业才能够扩张。当某一行业萎缩或者停止增产，并不必然意味着总体生产出现净减少。因为该行业萎缩所释放出劳工和资本，可供其他的行业用于扩张。简单地说，生产每一样东西，都得以牺牲放弃其他某样东西为代价。</p><h4 id="gt-“稳定”物价"><a href="#gt-“稳定”物价" class="headerlink" title="&gt; “稳定”物价"></a>&gt; “稳定”物价</h4><p>在自由竞争的市场经济中，由于价格下跌而被逐出市场的，都是高成本、低效率的生产者。政府一刀切限制生产的结果，意味着效率高、成本低的生产者不准以低价供应其全部产量，同时意味着效率低、成本高的生产者仍将被人为地保留在业内。这就使得产品的平均生产成本提高，生产效率下降。因为土地、人力和资本这些资源被无效率的边际生产者继续占用，就无法转移到收益和效率更高的地方。</p><p>政府规划官员在这里所说的自由贸易到底是什么意思，我弄不清楚，但是他们的言下之意不包含哪些事情，我们倒是相当确定。他们的意思绝对不是说，平民百姓可以自由地从事买卖或者借贷，自由谈判价格或利率，自由出入他们觉得收益最大的地方。他们的意思绝对不是说，平民百姓可以自主决定产量、可以自由移民、可以自由支配自己的资本和财物。我怀疑他们的意思是说，由官僚来替平民百姓决定这些事情，并且告诉平民，服从计划奖赏将是生活水平的提高。</p><h4 id="gt-政府管制价格"><a href="#gt-政府管制价格" class="headerlink" title="&gt; 政府管制价格"></a>&gt; 政府管制价格</h4><p>当商品价格被人为限制在它的市场价位之下时，不可避免地会带来两个结果。第一是导致受控商品的需求增加。由于该商品变得便宜，图便宜的人会更多，人们也买得起更多。第二是导致受控商品供给减少。由于人们买得更多，该商品势必供不应求。但是生产积极性却遭到打击，降价致使该商品收益率降低，甚至做不出利润。边际生产者被迫出局。即使最有效率的生产者也可能亏本经营。</p><p>价格上涨的真正原因，是商品匮乏或货币过剩。法定价格上限根本无法解决这两方面的问题。事实上，这只会加剧商品短缺。人们当然不想花更多的钱去购买牛奶、黄油、鞋子、家具、戏票、钻石，去支付房租。对于每个人来说，惟一该涨价的，是他自己生产的东西；这是一个人理解并赞成价格上涨的原因。不过，人人都觉得自己的产品才应该最应该涨价。</p><p>我们每个人都具备多重经济角色。每个人都是生产者、纳税人，消费者。作为生产者时，他希望涨价(因为顾及自己的服务或产品)；作为消费者时，他希望降价(因为顾及自己的钱口袋)。作为消费者，他可能主张或者默许政府实施补贴；作为纳税人，他反对政府实施补贴。每个人都心照不宣想从各种政治力量的斗争中获利，例如让自己产品涨价而不让原材料涨价，或者作为消费者支持价格管制。</p><h4 id="gt-房租管制的后果"><a href="#gt-房租管制的后果" class="headerlink" title="&gt; 房租管制的后果"></a>&gt; 房租管制的后果</h4><p>不合理的价格管制施加在面包等日用消费品上的时候，面包店只要停止烘焙、停止出售面包就行。面包马上就会供不应求，迫使政治人物提高价格上限，甚至取消价格管制。房租管制则有所不同。一方面，房东除了继续把房子租出去，通常没有别的选择，因为在扣除税金和抵押贷款利息之后总还有点净收入，尽管无力维修。另一方面，由于房屋经久耐用，可能要住上好几年，租户才会开始感受到房东无力维修的苦果。也许再过数年，人们才会意识到房荒与房屋设施恶化，与房租管制有直接的关系。在普通价格管制取消多年之后，房租管制仍在继续，因为政治人物知道租户的票数比房东的票数更多。</p><p>房租管制迟迟不能取消，其压力来自于只顾某个群体短期利益的那些人。如果我们考虑每个人(包括租户本身)受到的长期影响，我们就会认识到房租管制徒劳无益，就会认识到管制手段越严厉、实施时间越长，它的破坏性越大。</p><h4 id="gt-最低工资法"><a href="#gt-最低工资法" class="headerlink" title="&gt; 最低工资法"></a>&gt; 最低工资法</h4><p>我们已经看到政府以行政干预提高某些商品的价格所带来的破坏作用。通过法律规定最低工资标准来提高工资，也会造成类似的破坏。这不足为奇，因为工资其实就是劳工提供服务的价格。很不幸，这种价格竟然取了个与其他的价格迥然不同的名称，这会让人思维不清楚。</p><p>最低工资法的出发点是要消灭低工资，但事实表明，这样的法律不但在保护劳工方面效果有限，而且弊大于利的程度和其目标成正比。提高工资的最佳手段，是提高边际劳工的生产力。这可以通过许多方法来实现：通过增加资本累积，例如添置机器以协助劳工；通过引进新的发明和改进；通过提高管理效率；通过激励更加勤奋和更有效劳作；通过更好的教育培训等。</p><h4 id="gt-工会真的有办法提高工资吗？"><a href="#gt-工会真的有办法提高工资吗？" class="headerlink" title="&gt; 工会真的有办法提高工资吗？"></a>&gt; 工会真的有办法提高工资吗？</h4><p>工会会员不是慈善家和圣人，靠工资吃饭的劳工也不是。我们都渴望获得经济公平，但这种所谓的公平其实是，渴望自己拥有东西与比自己过得好的人一样多，而不希望比自己过得差的人拥有与自己一样多的东西。不过，我们现在不去关心这种人性上的劣根性，我们现在所关心的，是特定经济理论是否合乎逻辑和合理。</p><h4 id="gt-“有足够的钱买回商品”"><a href="#gt-“有足够的钱买回商品”" class="headerlink" title="&gt; “有足够的钱买回商品”"></a>&gt; “有足够的钱买回商品”</h4><p>经济均衡(equilibrium)的一般意义和影响的问题上。均衡工资和价格是指使供给和需求均衡时的工资和价格。如果通过政府或私人强行将价格提升到高于它们的均衡水平，需求将下降，从而导致生产萎缩。如果强行将价格压到低于它们的均衡水平，随之而来的利润的下降将意味着供应下降或生产缩减。所以任何迫使价格高于或低于均衡水平(自由市场经常使之趋于这个水平)的企图，都将导致就业量和生产量缩减到低于它们应有的水准。</p><p>价格、工资和利润，能决定产品分配。最好的价格并非最高价格，而是能促进达到最大产量和最大销售量的价格。最好的工资率也不是最高的工资率，而是能达到充分生产、充分就业和持久稳定的最大总工资的工资率。不论从业界还是从劳工的角度来看，最好的利润不是指最低的利润，而是能够鼓励更多人投资、创业，更多人成为雇主，能提供更多就业机会的利润。</p><h4 id="gt-利润的功能"><a href="#gt-利润的功能" class="headerlink" title="&gt; 利润的功能"></a>&gt; 利润的功能</h4><p>利润的一大功能是引导生产要素的流向，依照需求来配置各种商品的相对产出。利润的实现不是靠提高价格，而是靠降低生产成本，靠讲求经济效益和效率。利润来自成本和价格之间的关系，它不仅告诉我们生产何种产品最经济，而且告诉我们哪种生产方式最经济。</p><h4 id="gt-通货膨胀的幻景"><a href="#gt-通货膨胀的幻景" class="headerlink" title="&gt; 通货膨胀的幻景"></a>&gt; 通货膨胀的幻景</h4><p>通货膨胀一旦启动便不受控制。我们不可能引导其平缓温和地结束，我们无法扭转随之而来的经济萧条。甚至不可能在预先设定的时点，或在价格上涨到预先设定的水平而让通货膨胀暂停。靠政治力量和经济力量都无法驾驭通货膨胀。那些能从通货膨胀中获得利益的政治集团，必然坚持继续维持通货膨胀。</p><p>通货膨胀的真正作用是改变价格和成本之间的关系。它所带来的最大改变，是提高相对于工资率的商品价格，借此来恢复价格和生产成本之间可以持续经营的关系，进而能借恢复经营利润来盘活闲置资源，鼓励恢复生产。</p><p>通货膨胀给每一经济过程罩上了一层迷惑人的面纱，欺骗了几乎所有人，包括那些受其害的人。当我们听到，国民收入(以货币计量)比通货膨胀之前增加一倍，谁不会觉得更富有更骄傲呢？原先周薪75美元的小职员，现在能拿120美元，他也会觉得比以前更强。他当然能感觉到生活费用涨得厉害，但是他并没有充分觉察到自己的真实处境。虽然同样是实质购买力降低，他无法接受生活费用不变而工资调减的事实，但是他会欣然接受工资调增而生活费用上涨的事实。通货膨胀有如自我暗示、催眠术、麻醉剂，可以减轻手术时的痛苦。通货膨胀也像鸦片，吸上一口，就觉得自己无所不能。</p><h4 id="gt-抨击储蓄"><a href="#gt-抨击储蓄" class="headerlink" title="&gt; 抨击储蓄"></a>&gt; 抨击储蓄</h4><p>有些被认为是当今杰出经济学家的人，为了避免国家经济萧条，竟然抨击储蓄，鼓吹大肆消费。当有人对这些政策的长期效应提出质疑时，他们却像败家子讥讽父母的忠告那样不屑一顾地说：“何必看得那么远呢？要知道从长远来看，我们都是要死的。”这么一句轻率的戏言，却被人当作至理名言和大智慧，奉为经济生活中的信条。不幸的是，长远并不真的那么远，或远或近的过去实施的政策所带来的长期影响，已经给我们带来煎熬了。坏的经济学家昨天要我们置之不理的明天，转眼就成了今天。总之，现代世界中的“储蓄”，只是支出的另一种形式。两者的差别，通常在于前者把钱交给别人用于扩大生产。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;关于这堂课&quot;&gt;&lt;a href=&quot;#关于这堂课&quot; class=&quot;headerlink&quot; title=&quot;关于这堂课&quot;&gt;&lt;/a&gt;关于这堂课&lt;/h3&gt;&lt;p&gt;经济学的艺术，在于不仅要观察任何行为或整车的即期影响，更要考察比较长远的影响；不仅要关注政策对某个群体产生的影响，更要追踪对所有群体造成的影响。&lt;/p&gt;&lt;h3 id=&quot;课程的应用&quot;&gt;&lt;a href=&quot;#课程的应用&quot; class=&quot;headerlink&quot; title=&quot;课程的应用&quot;&gt;&lt;/a&gt;课程的应用&lt;/h3&gt;&lt;h4 id=&quot;gt-破橱窗&quot;&gt;&lt;a href=&quot;#gt-破橱窗&quot; class=&quot;headerlink&quot; title=&quot;&amp;gt; 破橱窗&quot;&gt;&lt;/a&gt;&amp;gt; 破橱窗&lt;/h4&gt;&lt;p&gt;话说一个顽童抡起砖头，砸破了面包店的橱窗。当店主怒气冲冲追出来时，小捣蛋早已逃之夭夭，只剩下一群看闹热的围观者。大家盯着橱窗的破洞以及四下散落的玻璃碎片，有些人开始互相议论，宽慰店主：玻璃破了很是可惜，可是这也有好的一面。这不，对面的玻璃店又有生意了。他们越琢磨越来劲：一面新的橱窗需要250美元。玻璃店多了250美元，会去别的商家那里消费，那些个商家的口袋里多了几个钱，又会向更多的商家买东西。经这么一说，小小一片破橱窗，竟能够连环不断提供资金给很多商家，使很多人获得就业机会。要是照这个逻辑推下去，结论便是：扔砖头的那个小捣蛋，不但不是社区的祸害，反而是造福社区的善人。&lt;/p&gt;
    
    </summary>
    
      <category term="Finance" scheme="http://hamstersi.github.io/categories/Finance/"/>
    
    
      <category term="Economics" scheme="http://hamstersi.github.io/tags/Economics/"/>
    
      <category term="Finance" scheme="http://hamstersi.github.io/tags/Finance/"/>
    
  </entry>
  
  <entry>
    <title>2013 - 11 浮生奥克兰</title>
    <link href="http://hamstersi.github.io/2017/02/2013-11-%E6%B5%AE%E7%94%9F%E5%A5%A5%E5%85%8B%E5%85%B0/"/>
    <id>http://hamstersi.github.io/2017/02/2013-11-浮生奥克兰/</id>
    <published>2017-02-18T10:41:07.000Z</published>
    <updated>2017-03-02T14:31:13.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p>和家里和朋友告了别，我是听着这首歌踏入中土世界的。飞机盘旋在奥克兰上空的时候，心里特别感动。大概是被这云层下的异乡与自以为是的勇气给触动了。其实在奥克兰待得真的不算长，不久便迫于生计辗转各个城市，最终在基督城安顿了半年。</p><p>对于基督城的感情，那大概是习惯使然。而对于新西兰来说，最有标志性的还是要数奥克兰和惠灵顿。在码头听海鸥和诗人游唱，在山上看日落看霓虹渐起，望天空塔向跨年烟火许愿，我对陌生人说新年要快乐。那个时候我不曾相问未来，朋友们萍水相逢来来去去换了一批又一批，终究却还是独自上路。</p><a id="more"></a><p>我一直特别想念这中土世界，从未断过的想念，一想到她呀，心里就悸动。那个时候少年轻狂，如今也只是活了个不明不白。让我们再见吧，奥克兰、再见吧，新西兰——</p><div id="aplayer0" class="aplayer" style="margin-bottom:20px;width:99%"></div><script>new APlayer({element:document.getElementById("aplayer0"),narrow:!1,autoplay:!1,showlrc:0,music:{title:"Song of the Irish Whistle",author:"Joanie Madden",url:"http://m2.music.126.net/W2p5D6q5l4XF17yCs1Ietw==/2098967697433470.mp3",pic:"http://wx4.sinaimg.cn/large/672d88aaly9fd34xh7c7kj20ti0ti1kx.jpg"}})</script><p><img src="http://wx2.sinaimg.cn/large/672d88aagy1fcutfciukmj23b4274b29.jpg" alt="Auckland | Ferry Terminal"><br><img src="http://wx3.sinaimg.cn/large/672d88aagy1fcutf5g35wj23b4274hdt.jpg" alt="Auckland | Ferry Terminal"><br><img src="http://wx3.sinaimg.cn/large/672d88aagy1fcuteo3qznj23b4274hbf.jpg" alt="Auckland | Mt. Eden"><br><img src="http://wx4.sinaimg.cn/large/672d88aagy1fcutey4l2vj23b4274hc8.jpg" alt="Auckland | Mt. Eden"><br><img src="http://wx1.sinaimg.cn/large/672d88aagy1fcutfgvtxmj23b42741kx.jpg" alt="Auckland | Sky Tower"><br><img src="http://wx4.sinaimg.cn/large/672d88aagy1fcutfo7pb1j23b4274e81.jpg" alt="Auckland | Sky Tower"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;和家里和朋友告了别，我是听着这首歌踏入中土世界的。飞机盘旋在奥克兰上空的时候，心里特别感动。大概是被这云层下的异乡与自以为是的勇气给触动了。其实在奥克兰待得真的不算长，不久便迫于生计辗转各个城市，最终在基督城安顿了半年。&lt;/p&gt;&lt;p&gt;对于基督城的感情，那大概是习惯使然。而对于新西兰来说，最有标志性的还是要数奥克兰和惠灵顿。在码头听海鸥和诗人游唱，在山上看日落看霓虹渐起，望天空塔向跨年烟火许愿，我对陌生人说新年要快乐。那个时候我不曾相问未来，朋友们萍水相逢来来去去换了一批又一批，终究却还是独自上路。&lt;/p&gt;
    
    </summary>
    
      <category term="Diary" scheme="http://hamstersi.github.io/categories/Diary/"/>
    
    
      <category term="Neverland" scheme="http://hamstersi.github.io/tags/Neverland/"/>
    
      <category term="Helloworld" scheme="http://hamstersi.github.io/tags/Helloworld/"/>
    
  </entry>
  
  <entry>
    <title>Porting the Essential Dynamics/Molecular Dynamics method for large-scale nucleic acid simulations to ARCHER</title>
    <link href="http://hamstersi.github.io/2017/02/Porting-the-Essential-Dynamics-Molecular-Dynamics-method-for-large-scale-nucleic-acid-simulations-to-ARCHER/"/>
    <id>http://hamstersi.github.io/2017/02/Porting-the-Essential-Dynamics-Molecular-Dynamics-method-for-large-scale-nucleic-acid-simulations-to-ARCHER/</id>
    <published>2017-02-18T10:26:07.000Z</published>
    <updated>2017-03-02T14:02:53.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p>The Molecular Dynamics (MD) simulation is an important computer modelling and simulation technique to study the physical behaviours of molecules and atoms. It has been widely applied in various scientific fields, and is of great value to do the MD simulation on biomacromolecules such as proteins and DNA, rendering scientists and researchers a rich source of information about the biological macromolecules.</p><p>The goal of this project was to develop a parallel code to perform the MD simulation on the large-scale nucleic acid DNA systems with a newly proposed method, namely the Essential Dynamics (ED), after reviewing two current existing code versions.<a id="more"></a> The two codes are written in Fortran 95 with MPI and Python respectively. Both codes have implemented the new Essential Dynamics/Molecular Dynamics (ED/MD) simulation for the DNA systems but in a quite primitive mode that contain serious limitations either on the execution flexibility (the Fortran 95/MPI code) or on the performance (the serial Python code). The new code is developed in C++ with MPI, which is a more flexible parallel code version compared to the Fortran 95/MPI code in the aspect of the degree of parallelism, while the performance is significantly improved by contrast with the serial Python code. The new code is compiled and run on ARCHER during the development, but it also has fairly good portability that can be executed on other parallel platforms or in personal laptops.</p><p>The simulation results of the new C++/MPI code are verified according to the results of the Fortran 95/MPI code, and the performance is also analysed afterwards. Due to the new parallelisation strategy that is different from the Fortran 95/MPI code, the improved flexibility of the parallelism is at the expense of some message passing overheads. To reduce the overheads and improve the performance, several parallel schemes have been proposed and implemented. The performance now is quite reasonable and acceptable with less time wasted on the communication. However, the future development work can still be carried on to further improve the execution efficiency of the parallel code.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The Molecular Dynamics (MD) simulation is an important computer modelling and simulation technique to study the physical behaviours of molecules and atoms. It has been widely applied in various scientific fields, and is of great value to do the MD simulation on biomacromolecules such as proteins and DNA, rendering scientists and researchers a rich source of information about the biological macromolecules.&lt;/p&gt;&lt;p&gt;The goal of this project was to develop a parallel code to perform the MD simulation on the large-scale nucleic acid DNA systems with a newly proposed method, namely the Essential Dynamics (ED), after reviewing two current existing code versions.
    
    </summary>
    
      <category term="Tech" scheme="http://hamstersi.github.io/categories/Tech/"/>
    
    
      <category term="HPC" scheme="http://hamstersi.github.io/tags/HPC/"/>
    
      <category term="MPI" scheme="http://hamstersi.github.io/tags/MPI/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://hamstersi.github.io/2017/02/Hello-World/"/>
    <id>http://hamstersi.github.io/2017/02/Hello-World/</id>
    <published>2017-02-14T14:43:20.000Z</published>
    <updated>2017-03-02T14:31:07.000Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><center><br>Hello World! This planet turns so fast, everything burns, ashes to ash, but for now you are mine…<br></center><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;&lt;br&gt;Hello World! This planet turns so fast, everything burns, ashes to ash, but for now you are mine…&lt;br&gt;&lt;/center&gt;
    
    </summary>
    
      <category term="Diary" scheme="http://hamstersi.github.io/categories/Diary/"/>
    
    
      <category term="Helloworld" scheme="http://hamstersi.github.io/tags/Helloworld/"/>
    
  </entry>
  
</feed>
