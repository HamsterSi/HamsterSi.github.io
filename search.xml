<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[GCC常用命令简介]]></title>
      <url>http://hamstersi.github.io/2017/08/GCC%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E7%AE%80%E4%BB%8B/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h1><p>GCC 的意思也只是 GNU C Compiler 而已。经过了这么多年的发展，GCC 已经不仅仅能支持 C 语言；它现在还支持 Ada 语言、C++ 语言、Java 语言、Objective C 语言、Pascal 语言、COBOL语言，以及支持函数式编程和逻辑编程的 Mercury 语言，等等。而 GCC 也不再单只是 GNU C 语言编译器的意思了，而是变成了 GNU Compiler Collection 也即是 GNU 编译器家族的意思了。另一方面，说到 GCC 对于操作系统平台及硬件平台支持，概括起来就是一句话：无所不在。</p><h1 id="2-简单编译"><a href="#2-简单编译" class="headerlink" title="2 简单编译"></a>2 简单编译</h1><p>示例程序如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//test.c</span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span></div><div class="line">&#123;</div><div class="line">    <span class="built_in">printf</span>(<span class="string">"Hello World!\n"</span>);</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure><a id="more"></a><p>这个程序，一步到位的编译指令是:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gcc test.c -o test</div></pre></td></tr></table></figure><p>实质上，上述编译过程是分为四个阶段进行的，即预处理(也称预编译，Preprocessing)、编译(Compilation)、汇编 (Assembly)和连接(Linking)。</p><h2 id="2-1-预处理"><a href="#2-1-预处理" class="headerlink" title="2.1 预处理"></a>2.1 预处理</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gcc -E test.c -o test.i 或 gcc -E test.c</div></pre></td></tr></table></figure><p>可以输出test.i文件中存放着test.c经预处理之后的代码。打开test.i文件，看一看，就明白了。后面那条指令，是直接在命令行窗口中输出预处理后的代码.</p><p>gcc的-E选项，可以让编译器在预处理后停止，并输出预处理结果。在本例中，预处理结果就是将stdio.h 文件中的内容插入到test.c中了。</p><h2 id="2-2-编译为汇编代码-Compilation"><a href="#2-2-编译为汇编代码-Compilation" class="headerlink" title="2.2 编译为汇编代码(Compilation)"></a>2.2 编译为汇编代码(Compilation)</h2><p>预处理之后，可直接对生成的test.i文件编译，生成汇编代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gcc -S test.i -o test.s</div></pre></td></tr></table></figure><p>gcc的-S选项，表示在程序编译期间，在生成汇编代码后，停止，-o输出汇编代码文件。</p><h2 id="2-3-汇编-Assembly"><a href="#2-3-汇编-Assembly" class="headerlink" title="2.3 汇编(Assembly)"></a>2.3 汇编(Assembly)</h2><p>对于上一小节中生成的汇编代码文件test.s，gas汇编器负责将其编译为目标文件，如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gcc -c test.s -o test.o</div></pre></td></tr></table></figure><h2 id="2-4-连接-Linking"><a href="#2-4-连接-Linking" class="headerlink" title="2.4 连接(Linking)"></a>2.4 连接(Linking)</h2><p>gcc连接器是gas提供的，负责将程序的目标文件与所需的所有附加的目标文件连接起来，最终生成可执行文件。附加的目标文件包括静态连接库和动态连接库。</p><p>对于上一小节中生成的test.o，将其与Ｃ标准输入输出库进行连接，最终生成程序test</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gcc test.o -o test</div></pre></td></tr></table></figure><p>在命令行窗口中，执行./test, 让它说HelloWorld吧！</p><h1 id="3-多个程序文件的编译"><a href="#3-多个程序文件的编译" class="headerlink" title="3 多个程序文件的编译"></a>3 多个程序文件的编译</h1><p>通常整个程序是由多个源文件组成的，相应地也就形成了多个编译单元，使用GCC能够很好地管理这些编译单元。假设有一个由test1.c和 test2.c两个源文件组成的程序，为了对它们进行编译，并最终生成可执行程序test，可以使用下面这条命令：</p><p>gcc test1.c test2.c -o test</p><p>如果同时处理的文件不止一个，GCC仍然会按照预处理、编译和链接的过程依次进行。如果深究起来，上面这条命令大致相当于依次执行如下三条命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">gcc -c test1.c -o test1.o</div><div class="line">gcc -c test2.c -o test2.o</div><div class="line">gcc test1.o test2.o -o test</div></pre></td></tr></table></figure><h1 id="4-检错"><a href="#4-检错" class="headerlink" title="4 检错"></a>4 检错</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gcc -pedantic illcode.c -o illcode</div></pre></td></tr></table></figure><p>-pedantic编译选项并不能保证被编译程序与ANSI/ISO C标准的完全兼容，它仅仅只能用来帮助Linux程序员离这个目标越来越近。或者换句话说，-pedantic选项能够帮助程序员发现一些不符合 ANSI/ISO C标准的代码，但不是全部，事实上只有ANSI/ISO C语言标准中要求进行编译器诊断的那些情况，才有可能被GCC发现并提出警告。</p><p>除了-pedantic之外，GCC还有一些其它编译选项也能够产生有用的警告信息。这些选项大多以-W开头，其中最有价值的当数-Wall了，使用它能够使GCC产生尽可能多的警告信息。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gcc -Wall illcode.c -o illcode</div></pre></td></tr></table></figure><p>GCC给出的警告信息虽然从严格意义上说不能算作错误，但却很可能成为错误的栖身之所。一个优秀的Linux程序员应该尽量避免产生警告信息，使自己的代码始终保持标准、健壮的特性。所以将警告信息当成编码错误来对待，是一种值得赞扬的行为！所以，在编译程序时带上-Werror选项，那么GCC会在所有产生警告的地方停止编译，迫使程序员对自己的代码进行修改，如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gcc -Werror test.c -o test</div></pre></td></tr></table></figure><h1 id="5-库文件连接"><a href="#5-库文件连接" class="headerlink" title="5 库文件连接"></a>5 库文件连接</h1><p>开发软件时，完全不使用第三方函数库的情况是比较少见的，通常来讲都需要借助许多函数库的支持才能够完成相应的功能。从程序员的角度看，函数库实际上就是一些头文件（.h）和库文件（so、或lib、dll）的集合。。虽然Linux下的大多数函数都默认将头文件放到/usr/include/目录下，而库文件则放到/usr/lib/目录下；Windows所使用的库文件主要放在Visual Stido的目录下的include和lib，以及系统文件夹下。但也有的时候，我们要用的库不再这些目录下，所以GCC在编译时必须用自己的办法来查找所需要的头文件和库文件。</p><p>例如我们的程序test.c是在linux上使用c连接mysql，这个时候我们需要去mysql官网下载MySQL Connectors的C库，下载下来解压之后，有一个include文件夹，里面包含mysql connectors的头文件，还有一个lib文件夹，里面包含二进制so文件libmysqlclient.so</p><p>其中inclulde文件夹的路径是/usr/dev/mysql/include,lib文件夹是/usr/dev/mysql/lib</p><h2 id="5-1-编译成可执行文件"><a href="#5-1-编译成可执行文件" class="headerlink" title="5.1 编译成可执行文件"></a>5.1 编译成可执行文件</h2><p>首先我们要进行编译test.c为目标文件，这个时候需要执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gcc –c –I /usr/dev/mysql/include test.c –o test.o</div></pre></td></tr></table></figure><h2 id="5-2-链接"><a href="#5-2-链接" class="headerlink" title="5.2 链接"></a>5.2 链接</h2><p>最后我们把所有目标文件链接成可执行文件:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gcc –L /usr/dev/mysql/lib –lmysqlclient test.o –o test</div></pre></td></tr></table></figure><p>Linux下的库文件分为两大类分别是动态链接库（通常以.so结尾）和静态链接库（通常以.a结尾），二者的区别仅在于程序执行时所需的代码是在运行时动态加载的，还是在编译时静态加载的。</p><h2 id="5-3-强制链接时使用静态链接库"><a href="#5-3-强制链接时使用静态链接库" class="headerlink" title="5.3 强制链接时使用静态链接库"></a>5.3 强制链接时使用静态链接库</h2><p>默认情况下， GCC在链接时优先使用动态链接库，只有当动态链接库不存在时才考虑使用静态链接库，如果需要的话可以在编译时加上-static选项，强制使用静态链接库。</p><p>在/usr/dev/mysql/lib目录下有链接时所需要的库文件libmysqlclient.so和libmysqlclient.a，为了让GCC在链接时只用到静态链接库，可以使用下面的命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gcc –L /usr/dev/mysql/lib –static –lmysqlclient test.o –o test</div></pre></td></tr></table></figure><p>静态库链接时搜索路径顺序：</p><ol><li>ld会去找GCC命令中的参数-L</li><li>再找gcc的环境变量LIBRARY_PATH</li><li>再找内定目录 /lib /usr/lib /usr/local/lib 这是当初compile gcc时写在程序内的</li></ol><p>动态链接时、执行时搜索路径顺序:</p><ol><li>编译目标代码时指定的动态库搜索路径</li><li>环境变量LD_LIBRARY_PATH指定的动态库搜索路径</li><li>配置文件/etc/ld.so.conf中指定的动态库搜索路径</li><li>默认的动态库搜索路径/lib</li><li>默认的动态库搜索路径/usr/lib</li></ol><p>有关环境变量：<br>LIBRARY_PATH环境变量：指定程序静态链接库文件搜索路径<br>LD_LIBRARY_PATH环境变量：指定程序动态链接库文件搜索路径</p>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Python读写CSV以及中文乱码]]></title>
      <url>http://hamstersi.github.io/2017/08/Python%E8%AF%BB%E5%86%99CSV%E4%BB%A5%E5%8F%8A%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h1 id="PANDAS包"><a href="#PANDAS包" class="headerlink" title="PANDAS包"></a>PANDAS包</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"></div><div class="line"><span class="comment">#任意的多组列表</span></div><div class="line">a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</div><div class="line">b = [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]    </div><div class="line"></div><div class="line"><span class="comment">#字典中的key值即为csv中列名</span></div><div class="line">dataframe = pd.DataFrame(&#123;<span class="string">'a_name'</span>:a,<span class="string">'b_name'</span>:b&#125;)</div><div class="line"></div><div class="line"><span class="comment">#将DataFrame存储为csv,index表示是否显示行名，default=True</span></div><div class="line">dataframe.to_csv(<span class="string">"test.csv"</span>,index=<span class="keyword">False</span>,sep=<span class="string">''</span>)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">   a_name  b_name</div><div class="line"><span class="number">0</span>       <span class="number">1</span>       <span class="number">4</span></div><div class="line"><span class="number">1</span>       <span class="number">2</span>       <span class="number">5</span></div><div class="line"><span class="number">2</span>       <span class="number">3</span>       <span class="number">6</span></div></pre></td></tr></table></figure><a id="more"></a><p>同样pandas也提供简单的读csv方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line">data = pd.read_csv(<span class="string">'test.csv'</span>)</div></pre></td></tr></table></figure><p>会得到一个DataFrame类型的data，不熟悉处理方法可以参考<a href="http://www.cnblogs.com/chaosimple/p/4153083.html" target="_blank" rel="external">pandas十分钟入门</a></p><h1 id="CSV包"><a href="#CSV包" class="headerlink" title="CSV包"></a>CSV包</h1><p>Python csv模块封装了常用的功能，使用的简单例子如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 读取csv文件</span></div><div class="line"><span class="keyword">import</span> csv</div><div class="line"><span class="keyword">with</span> open(<span class="string">'some.csv'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> f:        <span class="comment"># 采用b的方式处理可以省去很多问题</span></div><div class="line">    reader = csv.reader(f)</div><div class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> reader:</div><div class="line">        <span class="comment"># do something with row, such as row[0],row[1]</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">import</span> csv</div><div class="line"><span class="keyword">with</span> open(<span class="string">'some.csv'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:      <span class="comment"># 采用b的方式处理可以省去很多问题</span></div><div class="line">    writer = csv.writer(f)</div><div class="line">    writer.writerows(someiterable)</div></pre></td></tr></table></figure><p>默认的情况下, 读和写使用逗号做分隔符(delimiter)，用双引号作为引用符(quotechar)，当遇到特殊情况是，可以根据需要手动指定字符, 例如:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> csv</div><div class="line"><span class="keyword">with</span> open(<span class="string">'passwd'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</div><div class="line">    reader = csv.reader(f, delimiter=<span class="string">':'</span>, quoting=csv.QUOTE_NONE)</div><div class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> reader:</div><div class="line">        <span class="keyword">print</span> row</div></pre></td></tr></table></figure><p>上述示例指定冒号作为分隔符，并且指定quote方式为不引用。这意味着读的时候都认为内容是不被默认引用符(“)包围的。quoting的可选项为: QUOTE_ALL, QUOTE_MINIMAL, QUOTE_NONNUMERIC, QUOTE_NONE.</p><p>有点需要注意的是，当用writer写数据时， None 会被写成空字符串，浮点类型会被调用 repr() 方法转化成字符串。所以非字符串类型的数据会被 str() 成字符串存储。所以当涉及到unicode字符串时，可以自己手动编码后存储或者使用csv提供的 UnicodeWriter。</p><h1 id="CSV中文乱码"><a href="#CSV中文乱码" class="headerlink" title="CSV中文乱码"></a>CSV中文乱码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> csv</div><div class="line"><span class="keyword">import</span> codecs </div><div class="line">f = codecs.open(<span class="string">'temp.csv'</span>, <span class="string">'w'</span>, <span class="string">'utf_8_sig'</span>)  </div><div class="line">writer = csv.writer(f)  </div><div class="line">writer.writerow([<span class="string">'奥迪'</span>,<span class="string">'爱迪生'</span>,<span class="string">'方法'</span>])  </div><div class="line">f.close()</div></pre></td></tr></table></figure>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Python爬虫：一些常用的爬虫技巧总结]]></title>
      <url>http://hamstersi.github.io/2017/08/Python%E7%88%AC%E8%99%AB%EF%BC%9A%E4%B8%80%E4%BA%9B%E5%B8%B8%E7%94%A8%E7%9A%84%E7%88%AC%E8%99%AB%E6%8A%80%E5%B7%A7%E6%80%BB%E7%BB%93/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h1 id="1、基本抓取网页"><a href="#1、基本抓取网页" class="headerlink" title="1、基本抓取网页"></a>1、基本抓取网页</h1><p>GET和POST</p><pre class="hljs gradle"><code class="hljs gradle"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span>&nbsp;urllib2

url&nbsp;=&nbsp;<span class="hljs-string"><span class="hljs-string">"http://www.baidu.com"</span></span>
response&nbsp;=&nbsp;urllib2.urlopen(url)
<span class="hljs-keyword"><span class="hljs-keyword">print</span></span>&nbsp;response.<span class="hljs-keyword"><span class="hljs-keyword">read</span></span>()</code></pre><pre class="hljs gradle"><code class="hljs gradle"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span>&nbsp;urllib
<span class="hljs-keyword"><span class="hljs-keyword">import</span></span>&nbsp;urllib2

url&nbsp;=&nbsp;<span class="hljs-string"><span class="hljs-string">"http://abcde.com"</span></span>
form&nbsp;=&nbsp;{<span class="hljs-string"><span class="hljs-string">'name'</span></span>:<span class="hljs-string"><span class="hljs-string">'abc'</span></span>,<span class="hljs-string"><span class="hljs-string">'password'</span></span>:<span class="hljs-string"><span class="hljs-string">'1234'</span></span>}
form_data&nbsp;=&nbsp;urllib.urlencode(form)
request&nbsp;=&nbsp;urllib2.Request(url,form_data)
response&nbsp;=&nbsp;urllib2.urlopen(request)
<span class="hljs-keyword"><span class="hljs-keyword">print</span></span>&nbsp;response.<span class="hljs-keyword"><span class="hljs-keyword">read</span></span>()</code></pre><a id="more"></a><h1 id="2、使用代理IP"><a href="#2、使用代理IP" class="headerlink" title="2、使用代理IP"></a>2、使用代理IP</h1><p>在开发爬虫过程中经常会遇到IP被封掉的情况，这时就需要用到代理IP。在urllib2包中有ProxyHandler类，通过此类可以设置代理访问网页，如下代码片段：</p><pre class="hljs gradle"><code class="hljs gradle"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span>&nbsp;urllib2

proxy&nbsp;=&nbsp;urllib2.ProxyHandler({<span class="hljs-string"><span class="hljs-string">'http'</span></span>:&nbsp;<span class="hljs-string"><span class="hljs-string">'127.0.0.1:8087'</span></span>})
opener&nbsp;=&nbsp;urllib2.build_opener(proxy)
urllib2.install_opener(opener)
response&nbsp;=&nbsp;urllib2.urlopen(<span class="hljs-string"><span class="hljs-string">'http://www.baidu.com'</span></span>)
<span class="hljs-keyword"><span class="hljs-keyword">print</span></span>&nbsp;response.<span class="hljs-keyword"><span class="hljs-keyword">read</span></span>()</code></pre><h1 id="3、Cookies处理"><a href="#3、Cookies处理" class="headerlink" title="3、Cookies处理"></a>3、Cookies处理</h1><p>cookies是某些网站为了辨别用户身份、进行session跟踪而储存在用户本地终端上的数据(通常经过加密)，python提供了cookielib模块用于处理cookies，cookielib模块的主要作用是提供可存储cookie的对象，以便于与urllib2模块配合使用来访问Internet资源.</p><p>代码片段：</p><pre class="hljs scala"><code class="hljs scala"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span>&nbsp;urllib2,&nbsp;cookielib

cookie_support=&nbsp;urllib2.<span class="hljs-type"><span class="hljs-type">HTTPCookieProcessor</span></span>(cookielib.<span class="hljs-type"><span class="hljs-type">CookieJar</span></span>())
opener&nbsp;=&nbsp;urllib2.build_opener(cookie_support)
urllib2.install_opener(opener)
content&nbsp;=&nbsp;urllib2.urlopen(<span class="hljs-symbol"><span class="hljs-symbol">'http</span></span>:<span class="hljs-comment"><span class="hljs-comment">//XXXX').read()</span></span></code></pre><p>关键在于CookieJar()，它用于管理HTTP cookie值、存储HTTP请求生成的cookie、向传出的HTTP请求添加cookie的对象。整个cookie都存储在内存中，对CookieJar实例进行垃圾回收后cookie也将丢失，所有过程都不需要单独去操作。手动添加cookie:</p><pre class="hljs nginx"><code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">cookie</span></span>&nbsp;=&nbsp;<span class="hljs-string"><span class="hljs-string">"PHPSESSID=91rurfqm2329bopnosfu4fvmu7;&nbsp;kmsign=55d2c12c9b1e3;&nbsp;KMUID=b6Ejc1XSwPq9o756AxnBAg="</span></span>
request.add_header(<span class="hljs-string"><span class="hljs-string">"Cookie"</span></span>,&nbsp;cookie)</code></pre><h1 id="4、伪装成浏览器"><a href="#4、伪装成浏览器" class="headerlink" title="4、伪装成浏览器"></a>4、伪装成浏览器</h1><p>某些网站反感爬虫的到访，于是对爬虫一律拒绝请求。所以用urllib2直接访问网站经常会出现HTTP Error 403: Forbidden的情况.对有些 header 要特别留意，Server 端会针对这些 header 做检查:</p><p>&nbsp; 1.User-Agent 有些 Server 或 Proxy 会检查该值，用来判断是否是浏览器发起的 Request</p><p>&nbsp; 2.Content-Type 在使用 REST 接口时，Server 会检查该值，用来确定 HTTP Body 中的内容该怎样解析。</p><p>这时可以通过修改http包中的header来实现，代码片段如下：</p><pre class="hljs gradle"><code class="hljs gradle"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span>&nbsp;urllib2

headers&nbsp;=&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string"><span class="hljs-string">'User-Agent'</span></span>:<span class="hljs-string"><span class="hljs-string">'Mozilla/5.0&nbsp;(Windows;&nbsp;U;&nbsp;Windows&nbsp;NT&nbsp;6.1;&nbsp;en-US;&nbsp;rv:1.9.1.6)&nbsp;Gecko/20091201&nbsp;Firefox/3.5.6'</span></span>
}
request&nbsp;=&nbsp;urllib2.Request(
&nbsp;&nbsp;&nbsp;&nbsp;url&nbsp;=&nbsp;<span class="hljs-string"><span class="hljs-string">'http://my.oschina.net/jhao104/blog?catalog=3463517'</span></span>,
&nbsp;&nbsp;&nbsp;&nbsp;headers&nbsp;=&nbsp;headers
)
<span class="hljs-keyword"><span class="hljs-keyword">print</span></span>&nbsp;urllib2.urlopen(request).<span class="hljs-keyword"><span class="hljs-keyword">read</span></span>()</code></pre><h1 id="5、页面解析"><a href="#5、页面解析" class="headerlink" title="5、页面解析"></a>5、页面解析</h1><p>对于页面解析最强大的当然是正则表达式，这个对于不同网站不同的使用者都不一样，就不用过多的说明，附两个比较好的网址：</p><p>正则表达式入门：<a href="http://www.cnblogs.com/huxi/archive/2010/07/04/1771073.html" rel="external" target="_blank">http://www.cnblogs.com/huxi/archive/2010/07/04/1771073.html</a></p><p>正则表达式在线测试：<a href="http://tool.oschina.net/regex/" rel="external" target="_blank">http://tool.oschina.net/regex/</a>&amp;</p><p>其次就是解析库了，常用的有两个lxml和BeautifulSoup，对于这两个的使用介绍两个比较好的网站：</p><p>lxml：<a href="http://my.oschina.net/jhao104/blog/639448" rel="external" target="_blank">http://my.oschina.net/jhao104/blog/639448</a></p><p>BeautifulSoup：<a href="http://cuiqingcai.com/1319.html" rel="external" target="_blank">http://cuiqingcai.com/1319.html</a></p><p>对于这两个库，我的评价是，都是HTML/XML的处理库，Beautifulsoup纯python实现，效率低，但是功能实用，比如能用通过结果搜索获得某个HTML节点的源码；lxmlC语言编码，高效，支持Xpath</p><h1 id="6、验证码的处理"><a href="#6、验证码的处理" class="headerlink" title="6、验证码的处理"></a>6、验证码的处理</h1><p>对于一些简单的验证码，可以进行简单的识别。本人也只进行过一些简单的验证码识别。但是有些反人类的验证码，比如12306，可以通过打码平台进行人工打码，当然这是要付费的。</p><h1 id="7、gzip压缩"><a href="#7、gzip压缩" class="headerlink" title="7、gzip压缩"></a>7、gzip压缩</h1><p>有没有遇到过某些网页，不论怎么转码都是一团乱码。哈哈，那说明你还不知道许多web服务具有发送压缩数据的能力，这可以将网络线路上传输的大量数据消减 60% 以上。这尤其适用于 XML web 服务，因为 XML 数据 的压缩率可以很高。</p><p>但是一般服务器不会为你发送压缩数据，除非你告诉服务器你可以处理压缩数据。</p><p>于是需要这样修改代码：</p><pre class="hljs kotlin"><code class="hljs kotlin"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span>&nbsp;urllib2,&nbsp;httplib
request&nbsp;=&nbsp;urllib2.Request(<span class="hljs-string"><span class="hljs-string">'http://xxxx.com'</span></span>)
request.add_header(<span class="hljs-string"><span class="hljs-string">'Accept-encoding'</span></span>,&nbsp;<span class="hljs-string"><span class="hljs-string">'gzip'</span></span>)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-number"><span class="hljs-number">1</span></span>
opener&nbsp;=&nbsp;urllib2.build_opener()
f&nbsp;=&nbsp;opener.<span class="hljs-keyword"><span class="hljs-keyword">open</span></span>(request)</code></pre>这是关键:创建Request对象，添加一个 Accept-encoding 头信息告诉服务器你能接受 gzip 压缩数据 然后就是解压缩数据：<pre class="hljs gradle"><code class="hljs gradle"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span>&nbsp;StringIO
<span class="hljs-keyword"><span class="hljs-keyword">import</span></span>&nbsp;gzip

compresseddata&nbsp;=&nbsp;f.<span class="hljs-keyword"><span class="hljs-keyword">read</span></span>()&nbsp;
compressedstream&nbsp;=&nbsp;StringIO.StringIO(compresseddata)
gzipper&nbsp;=&nbsp;gzip.GzipFile(fileobj=compressedstream)&nbsp;
<span class="hljs-keyword"><span class="hljs-keyword">print</span></span>&nbsp;gzipper.<span class="hljs-keyword"><span class="hljs-keyword">read</span></span>()</code></pre><h1 id="8、多线程并发抓取"><a href="#8、多线程并发抓取" class="headerlink" title="8、多线程并发抓取"></a>8、多线程并发抓取</h1><p>单线程太慢的话，就需要多线程了，这里给个简单的线程池模板 这个程序只是简单地打印了1-10，但是可以看出是并发的。</p><p>虽然说python的多线程很鸡肋，但是对于爬虫这种网络频繁型，还是能一定程度提高效率的。</p><pre class="hljs python"><code class="hljs python"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span>&nbsp;threading&nbsp;<span class="hljs-keyword"><span class="hljs-keyword">import</span></span>&nbsp;Thread
<span class="hljs-keyword"><span class="hljs-keyword">from</span></span>&nbsp;Queue&nbsp;<span class="hljs-keyword"><span class="hljs-keyword">import</span></span>&nbsp;Queue
<span class="hljs-keyword"><span class="hljs-keyword">from</span></span>&nbsp;time&nbsp;<span class="hljs-keyword"><span class="hljs-keyword">import</span></span>&nbsp;sleep
<span class="hljs-comment"><span class="hljs-comment">#&nbsp;q是任务队列</span></span>
<span class="hljs-comment"><span class="hljs-comment">#NUM是并发线程总数</span></span>
<span class="hljs-comment"><span class="hljs-comment">#JOBS是有多少任务</span></span>
q&nbsp;=&nbsp;Queue()
NUM&nbsp;=&nbsp;<span class="hljs-number"><span class="hljs-number">2</span></span>
JOBS&nbsp;=&nbsp;<span class="hljs-number"><span class="hljs-number">10</span></span>
<span class="hljs-comment"><span class="hljs-comment">#具体的处理函数，负责处理单个任务</span></span>
<span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function">&nbsp;</span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">do_somthing_using</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(arguments)</span></span></span><span class="hljs-function">:</span></span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword"><span class="hljs-keyword">print</span></span>&nbsp;arguments
<span class="hljs-comment"><span class="hljs-comment">#这个是工作进程，负责不断从队列取数据并处理</span></span>
<span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function">&nbsp;</span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">working</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword"><span class="hljs-keyword">while</span></span>&nbsp;<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arguments&nbsp;=&nbsp;q.get()
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;do_somthing_using(arguments)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sleep(<span class="hljs-number"><span class="hljs-number">1</span></span>)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;q.task_done()
<span class="hljs-comment"><span class="hljs-comment">#fork&nbsp;NUM个线程等待队列</span></span>
<span class="hljs-keyword"><span class="hljs-keyword">for</span></span>&nbsp;i&nbsp;<span class="hljs-keyword"><span class="hljs-keyword">in</span></span>&nbsp;range(NUM):
&nbsp;&nbsp;&nbsp;&nbsp;t&nbsp;=&nbsp;Thread(target=working)
&nbsp;&nbsp;&nbsp;&nbsp;t.setDaemon(<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)
&nbsp;&nbsp;&nbsp;&nbsp;t.start()
<span class="hljs-comment"><span class="hljs-comment">#把JOBS排入队列</span></span>
<span class="hljs-keyword"><span class="hljs-keyword">for</span></span>&nbsp;i&nbsp;<span class="hljs-keyword"><span class="hljs-keyword">in</span></span>&nbsp;range(JOBS):
&nbsp;&nbsp;&nbsp;&nbsp;q.put(i)
<span class="hljs-comment"><span class="hljs-comment">#等待所有JOBS完成</span></span>
q.join()</code></pre><p>转载自：开源中国&nbsp;<a href="http://my.oschina.net/jhao104/blog/647308" rel="external" target="_blank">http://my.oschina.net/jhao104/blog/647308</a>&nbsp;</p>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[水鬼与夏天的葬礼]]></title>
      <url>http://hamstersi.github.io/2017/08/%E6%B0%B4%E9%AC%BC%E4%B8%8E%E5%A4%8F%E5%A4%A9%E7%9A%84%E8%91%AC%E7%A4%BC/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="水鬼"><a href="#水鬼" class="headerlink" title="-水鬼-"></a>-水鬼-</h2><p><img src="http://wx3.sinaimg.cn/mw690/672d88aagy1fibem5ku6wj21c50u3qad.jpg" alt="百鬼夜行"></p><p>河里是有水鬼的。那是一种身体类似蟒蛇，长有两只手的生物，全身上下布满绿色的腐皮和水泡，长着或尖锐或锈钝的金属牙齿，眼睛会发出橙黄色的荧光。眼角旁生着两条又细又长的绿色须毛，水鬼靠长须在水中游动。</p><a id="more"></a><p>不论如何，你若是遇上了，就是死路一条。他们可以喷出携带剧毒的黏液，或是直接用牙齿啃咬你瘦弱的肌骨。拖至河里吞食。</p><p>水鬼也不是必须要吃东西的，他们光靠水面上的浮萍和水葫芦也一样能生存地好好地。但是人的血液、骨肉、毛发，是他们的营养品。绝不如浮萍和水葫芦那般嚼着青涩无味，而是又腥又甜，甘醇香厚。噬一口能产生美妙的幻像，那是令水鬼颤栗的甜品。</p><p>所以他们从原本藏匿的淤泥石块里钻出来，潜伏在石岸边。有时候会突然从深绿色的河面里伸出皱皱巴巴如干尸般的绿手，迅速将你拽入水中，美餐一顿。</p><p>他们最喜爱吃幼童。幼童的肉体嫩滑软糯，毛发乌黑柔顺，连小指甲都是光滑干净的。他们的心脏里没像成人，暗埋着诸多阴险与贪念，人类肮脏污秽的本性，在他们身上还未完全发育好。</p><p>水鬼不喜欢阳光，像他们那样做着偷窃行为的强盗，是不能见光的。不是生理上的禁止，是精神上的。若是在那样明晃晃的光线下，水鬼会感觉全身上下的一切都被他人探照地完完全全。他们有头脑，然而又单细胞。明明所有人都晓得他们那点儿勾当，他们还认为一切是自己悄悄策划的私人秘密。其实如若碰到阳光，他们顶多是觉着眼睛酸痛，晒掉点儿死皮。</p><p>他们是群体，共生共存，观念和智商也一起进步着，或退化着。品性和爱好也如此。他们不能单独行动一片河域，即使有再多的人肉资源，如果水鬼单独生活，也活不过2天。但他们又是个体，因为自私的他们不愿意将自己辛苦得手的猎物让与其他渴望的同类。有些水鬼，一生都吃不着一个人，有些水鬼，日日夜夜口腔都浸泡在腥甜的人类血液之中。</p><p>水鬼群体不会永远只驻留在一个村庄，他们会分流，分群，各奔东西。也会在偶然的一天汇聚在一起，不论如何，他们都是同类，都是一起存活着相同的生物。他们互相憎恶着，嫉妒着，嘲笑着。呼吸着不同的空气，但生活在同样的腐朽的世界。</p><p>水鬼不是没有善良过的。</p><p>他们曾经驻留在深深的河水泥层里，与黑暗作伴，他们的思维却未曾被黑暗涤荡，他们总是在夜深人静的黑夜浮出水面，望望月亮，望望星星，望望夜灯，吹一吹凉爽的风，听一听昆虫的低语。采一朵青嫩的浮萍，游入自己的小窝静静啃啮。那个时候，水鬼没有那么泛滥。</p><p>他们甚至有一种帮助人类的本职，如果有孩子不慎坠入河里，他们会第一时间赶到然后迅速将孩子们送到岸边，确认无危险之后才肯离去。</p><p>但是他们时常被大人们追逐着伤害。</p><p>“那他妈是什么鬼东西！？”有些人见水鬼可怕的模样就抄起家伙狠狠揍一顿。水鬼通常挨这一顿揍，就离死不远了。</p><p>明明水鬼那么努力地隐藏自己，又那么努力地保护人类。</p><p>人类还是怀着恶意去探索，只因为他人口中的传言。他们见到水鬼了，兴奋又害怕，打死水鬼了，就产生一种满满的自豪感。人类总是缺少茶余饭后的谈资，因此不断挖掘他们心中所认为的丑恶，来体现智慧和丰满的天性。</p><p>所以水鬼变了。</p><p>坏一点儿，也没什么不好，再他们吃了第一具人类的躯体之后，他们这么想。</p><p>终于，那被数代水鬼压抑了几百年的黑暗细胞触及到滚烫的血液，猛烈地燃烧了，燃烧过后残存的黑色灰烬与骸骨，便是如今的他们所携带的品性。</p><p>但水鬼还是清清楚楚知道的，自己所做的一切都是邪恶的，可怕的。不像那么些人类，做着愚昧荒淫的事儿，还心安理得。</p><p>人类伤害着水鬼，水鬼伤害着人类，</p><p>人类和水鬼，或许也是相互依存的。</p><h2 id="夏天是一场葬礼"><a href="#夏天是一场葬礼" class="headerlink" title="夏天是一场葬礼"></a>夏天是一场葬礼</h2><p>它烹煮了一汪汤水，混搅了夏虫透而薄的翅子，森林之中斑驳的凉阴，浮于河滩之上细碎的光晕，全都在温度之中沸腾了，这是缄默的仪式，这是圣水。</p><p>夏蝉伏在古树虬结了细茎与疮痂的皮层上，不休不眠哀悼着，即便是并不庄严的喉咙，扰人的聒噪里夹杂着凄寂，在黎明的骤冷里化为动也不动的另一具具尸骸，和梗塞了的冷却的声音一起，被土壤掩埋。这是奏响的哀歌。</p><p>以及墓地，什么是墓地。哪里可以埋葬整整一个繁华的春天，流淌着化雪的河水，三更夜里杏雨拂窗，梨云遮月，桃花映面，堆砌起一整个春天。</p><p>地面才浓抹了胭脂水粉，却等那白驹走完这条巷道，又要被圣水洗去。</p><p>但是夏光，会为它画眉。</p><p>烈日将它焚成了灰，烟花祭的夜晚盛放。</p><p>然后海潮，卷起最后的遗灰。</p><p>春天的灵魂就在黄昏里飞走了。</p><p>等秋天来了，冬天过去。</p><p><strong>Poem- (<a href="http://fjisheng.lofter.com" target="_blank" rel="external">http://fjisheng.lofter.com</a>)</strong></p>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[2014 - 09 New Brighton @ Christchurch.md]]></title>
      <url>http://hamstersi.github.io/2017/03/2014-09-New-Brighton-@-Christchurch/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width:100%"><img src="http://wx4.sinaimg.cn/large/672d88aagy1fd8uvir0alj23b41v0npd.jpg" alt="New Brighton @ Christchurch"></div></div><div class="group-picture-row"><div class="group-picture-column" style="width:33.333333333333336%"><img src="http://wx1.sinaimg.cn/mw690/672d88aagy1fd8uv0rhn8j23b41v0npd.jpg" alt="New Brighton @ Christchurch"></div><div class="group-picture-column" style="width:33.333333333333336%"><img src="http://wx1.sinaimg.cn/mw690/672d88aagy1fd8uvdjjyyj23b41v0e81.jpg" alt="New Brighton @ Christchurch"></div><div class="group-picture-column" style="width:33.333333333333336%"><img src="http://wx2.sinaimg.cn/mw690/672d88aagy1fd8uuw8bghj23b41v0kjl.jpg" alt="New Brighton @ Christchurch"></div></div></div></div>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[CPU Metrics Reference - General Exploration]]></title>
      <url>http://hamstersi.github.io/2017/03/CPU-Metrics-Reference-General-Exploration/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p>To make your applications take advantage of CPU microarchitectures, you need to know how the application is utilizing available hardware resources. One way to obtain this knowledge is by using on-chip Performance Monitoring Units (PMUs). PMUs are dedicated pieces of logic within a CPU core that count specific hardware events as they occur on the system. Examples of these events may be Cache Misses or Branch Mispredictions. These events can be observed and combined to create useful high-level metrics such as Cycles per Instruction (CPI).</p><a id="more"></a><p><img src="http://wx1.sinaimg.cn/mw690/672d88aagy1fd8s0n9jsqj219r0c4n00.jpg" alt="Pipeline"></p><p>The pipeline of a modern high-performance CPU is quite complex. In the simplified view blow, the pipeline is divided conceptually into two halves, the Front-end and the Back-end. The Front-end is responsible for fetching the program code represented in architectural instructions and decoding them into one or more low-level hardware operations called <em>micro-ops</em> (uOps). The uOps are then fed to the Back-end in a process called <em>allocation</em>. Once allocated, the Back-end is responsible for monitoring when uOp’s data operands are available and executing the uOp in an available execution unit. The completion of a uOp’s execution is called <em>retirement</em>, and is where results of the uOp are committed to the architectural state (CPU registers or written back to memory). Usually, most uOps pass completely through the pipeline and retire, but sometimes speculatively fetched uOps may get cancelled before retirement – like in the case of mispredicted branches.<br><img src="http://wx4.sinaimg.cn/large/672d88aagy1fd8sap5wrqg20xc0h2jsp.gif" alt="uOp Categories"></p><h1 id="Front-End-Bound"><a href="#Front-End-Bound" class="headerlink" title="Front-End Bound"></a>Front-End Bound</h1><p>Superscalar processors can be conceptually divided into the ‘front-end’, where instructions are fetched and decoded into the operations that constitute them; and the ‘back-end’, where the required computation is performed. Each cycle, the front-end generates up to four of these operations placed into pipeline slots that then move through the back-end. Thus, for a given execution duration in clock cycles, it is easy to determine the maximum number of pipeline slots containing useful work that can be retired in that duration. The actual number of retired pipeline slots containing useful work, though, rarely equals this maximum. This can be due to several factors: some pipeline slots cannot be filled with useful work, either because the front-end could not fetch or decode instructions in time (‘Front-end bound’ execution) or because the back-end was not prepared to accept more operations of a certain kind (‘Back-end bound’ execution). Moreover, even pipeline slots that do contain useful work may not retire due to bad speculation. Front-end bound execution may be due to a large code working set, poor code layout, or microcode assists. Back-end bound execution may be due to long-latency operations or other contention for execution resources. Bad speculation is most frequently due to branch misprediction.</p><p>A significant proportion of pipeline slots are remaining empty. Possible reasons include a large code working size, poor code layout (requiring too many memory accesses per cycle to get sufficient instructions to fill four pipeline slots), or microcode assists.</p><h2 id="gt-Front-End-Latency"><a href="#gt-Front-End-Latency" class="headerlink" title="&gt; Front-End Latency"></a>&gt; Front-End Latency</h2><p>This metric represents a fraction of slots during which CPU was stalled due to front-end latency issues, such as instruction-cache misses, ITLB misses or fetch stalls after a branch misprediction. In such cases, the front-end delivers no uOps.</p><h3 id="gt-gt-ICache-Misses"><a href="#gt-gt-ICache-Misses" class="headerlink" title="&gt;&gt; ICache Misses"></a>&gt;&gt; ICache Misses</h3><p>To introduce new uOps into the pipeline, the core must either fetch them from a decoded instruction cache, or fetch the instructions themselves from memory and then decode them. In the latter path, the requests to memory first go through the L1I (level 1 instruction) cache that caches the recent code working set. Front-end stalls can accrue when fetched instructions are not present in the L1I. Possible reasons are a large code working set or fragmentation between hot and cold code. In the latter case, when a hot instruction is fetched into the L1I, any cold code on its cache line is brought along with it. This may result in the eviction of other, hotter code.</p><h3 id="gt-gt-ITLB-Overhead"><a href="#gt-gt-ITLB-Overhead" class="headerlink" title="&gt;&gt; ITLB Overhead"></a>&gt;&gt; ITLB Overhead</h3><p>In x86 architectures, mappings between virtual and physical memory are facilitated by a page table, which is kept in memory. To minimize references to this table, recently-used portions of the page table are cached in a hierarchy of ‘translation look-aside buffers’, or TLBs, which are consulted on every virtual address translation. As with data caches, the farther a request has to go to be satisfied, the worse the performance impact. This metric estimates the performance penalty of page walks induced on ITLB (instruction TLB) misses.</p><h3 id="gt-gt-Branch-Resteers"><a href="#gt-gt-Branch-Resteers" class="headerlink" title="&gt;&gt; Branch Resteers"></a>&gt;&gt; Branch Resteers</h3><p>This metric represents cycles fraction the CPU was stalled due to Branch Resteers. Branch Resteers estimates the Frontend delay in fetching operations from corrected path, following all sorts of mispredicted branches. For example, branchy code with lots of misprediction might get categorized under Branch Resteers. Note the value of this node may overlap with its siblings.</p><h3 id="gt-gt-DSB-Switches"><a href="#gt-gt-DSB-Switches" class="headerlink" title="&gt;&gt; DSB Switches"></a>&gt;&gt; DSB Switches</h3><p>Intel microarchitecture code name Sandy Bridge introduces a new decoded ICache. This cache, called the DSB (Decoded Stream Buffer), stores uOps that have already been decoded, avoiding many of the penalties of the legacy decode pipeline, called the MITE (Micro-instruction Translation Engine). However, when control flows out of the region cached in the DSB, the front-end incurs a penalty as uOp issue switches from the DSB to the MITE. This metric measures this penalty.</p><h3 id="gt-gt-Length-Changing-Prefixes"><a href="#gt-gt-Length-Changing-Prefixes" class="headerlink" title="&gt;&gt; Length Changing Prefixes"></a>&gt;&gt; Length Changing Prefixes</h3><p>This metric represents a fraction of cycles during which CPU was stalled due to Length Changing Prefixes (LCPs). To avoid this issue, use proper compiler flags. Intel Compiler enables these flags by default.</p><h3 id="gt-gt-MS-Switches"><a href="#gt-gt-MS-Switches" class="headerlink" title="&gt;&gt; MS Switches"></a>&gt;&gt; MS Switches</h3><p>This metric represents a fraction of cycles when the CPU was stalled due to switches of uOp delivery to the Microcode Sequencer (MS). Commonly used instructions are optimized for delivery by the DSB or MITE pipelines. Certain operations cannot be handled natively by the execution pipeline, and must be performed by microcode (small programs injected into the execution stream). Switching to the MS too often can negatively impact performance. The MS is designated to deliver long uOp flows required by CISC instructions like CPUID, or uncommon conditions like Floating Point Assists when dealing with Denormals.</p><h2 id="gt-Front-End-Bandwidth"><a href="#gt-Front-End-Bandwidth" class="headerlink" title="&gt; Front-End Bandwidth"></a>&gt; Front-End Bandwidth</h2><p>This metric represents a fraction of slots during which CPU was stalled due to front-end bandwidth issues, such as inefficiencies in the instruction decoders or code restrictions for caching in the DSB (decoded uOps cache). In such cases, the front-end typically delivers a non-optimal amount of uOps to the back-end.</p><h3 id="gt-gt-Front-End-Bandwidth-MITE"><a href="#gt-gt-Front-End-Bandwidth-MITE" class="headerlink" title="&gt;&gt; Front-End Bandwidth MITE"></a>&gt;&gt; Front-End Bandwidth MITE</h3><p>This metric represents a fraction of cycles during which CPU was stalled due to the MITE fetch pipeline issues, such as inefficiencies in the instruction decoders.</p><h3 id="gt-gt-Front-End-Bandwidth-DSB"><a href="#gt-gt-Front-End-Bandwidth-DSB" class="headerlink" title="&gt;&gt; Front-End Bandwidth DSB"></a>&gt;&gt; Front-End Bandwidth DSB</h3><p>This metric represents a fraction of cycles during which CPU was likely limited due to DSB (decoded uOp cache) fetch pipeline. For example, inefficient utilization of the DSB cache structure or bank conflict when reading from it, are categorized here.</p><h3 id="gt-gt-Front-End-Bandwidth-LSD"><a href="#gt-gt-Front-End-Bandwidth-LSD" class="headerlink" title="&gt;&gt; Front-End Bandwidth LSD"></a>&gt;&gt; Front-End Bandwidth LSD</h3><p>This metric represents a fraction of cycles during which CPU operation was limited by the LSD (Loop Stream Detector) unit. Typically, LSD provides good uOp supply. However, in some rare cases, optimal uOp delivery cannot be reached for small loops whose size (in terms of number of uOps) does not suit well the LSD structure.</p><h1 id="Bad-Speculation"><a href="#Bad-Speculation" class="headerlink" title="Bad Speculation"></a>Bad Speculation</h1><h2 id="gt-Branch-Mispredict"><a href="#gt-Branch-Mispredict" class="headerlink" title="&gt; Branch Mispredict"></a>&gt; Branch Mispredict</h2><p>When a branch mispredicts, some instructions from the mispredicted path still move through the pipeline. All work performed on these instructions is wasted since they would not have been executed had the branch been correctly predicted. This metric represents slots fraction the CPU has wasted due to Branch Misprediction. These slots are either wasted by uOps fetched from an incorrectly speculated program path, or stalls when the out-of-order part of the machine needs to recover its state from a speculative path.</p><h2 id="gt-Machine-Clears"><a href="#gt-Machine-Clears" class="headerlink" title="&gt; Machine Clears"></a>&gt; Machine Clears</h2><p>Certain events require the entire pipeline to be cleared and restarted from just after the last retired instruction. This metric measures three such events: memory ordering violations, self-modifying code, and certain loads to illegal address ranges. Machine Clears metric represents slots fraction the CPU has wasted due to Machine Clears. These slots are either wasted by uOps fetched prior to the clear, or stalls the out-of-order portion of the machine needs to recover its state after the clear.</p><h1 id="Back-End-Bound"><a href="#Back-End-Bound" class="headerlink" title="Back-End Bound"></a>Back-End Bound</h1><p>Identify slots where no uOps are delivered due to a lack of required resources for accepting more uOps in the back-end of the pipeline. Back-end metrics describe a portion of the pipeline where the out-of-order scheduler dispatches ready uOps into their respective execution units, and, once completed, these uOps get retired according to program order. Stalls due to data-cache misses or stalls due to the overloaded divider unit are examples of back-end bound issues.</p><h2 id="gt-Memory-Bound"><a href="#gt-Memory-Bound" class="headerlink" title="&gt; Memory Bound"></a>&gt; Memory Bound</h2><p>This metric shows how memory subsystem issues affect the performance. Memory Bound measures a fraction of slots where pipeline could be stalled due to demand load or store instructions. This accounts mainly for incomplete in-flight memory demand loads that coincide with execution starvation in addition to less common cases where stores could imply back-pressure on the pipeline.</p><h3 id="gt-gt-L1-Bound"><a href="#gt-gt-L1-Bound" class="headerlink" title="&gt;&gt; L1 Bound"></a>&gt;&gt; L1 Bound</h3><p>This metric shows how often machine was stalled without missing the L1 data cache. The L1 cache typically has the shortest latency. However, in certain cases like loads blocked on older stores, a load might suffer a high latency even though it is being satisfied by the L1.</p><h4 id="gt-gt-gt-DTLB-Overhead"><a href="#gt-gt-gt-DTLB-Overhead" class="headerlink" title="&gt;&gt;&gt; DTLB Overhead"></a>&gt;&gt;&gt; DTLB Overhead</h4><p>This metric estimates the performance penalty paid for missing the first-level data TLB (DTLB) that includes hitting in the second-level data TLB (STLB) as well as performing a hardware page walk on an STLB miss.</p><h4 id="gt-gt-gt-Loads-Blocked-by-Store-Forwarding"><a href="#gt-gt-gt-Loads-Blocked-by-Store-Forwarding" class="headerlink" title="&gt;&gt;&gt; Loads Blocked by Store Forwarding"></a>&gt;&gt;&gt; Loads Blocked by Store Forwarding</h4><p>To streamline memory operations in the pipeline, a load can avoid waiting for memory if a prior store, still in flight, is writing the data that the load wants to read (a ‘store forwarding’ process). However, in some cases, generally when the prior store is writing a smaller region than the load is reading, the load is blocked for a significant time pending the store forward. This metric measures the performance penalty of such blocked loads.</p><h4 id="gt-gt-gt-Lock-Latency"><a href="#gt-gt-gt-Lock-Latency" class="headerlink" title="&gt;&gt;&gt; Lock Latency"></a>&gt;&gt;&gt; Lock Latency</h4><p>This metric represents cycles fraction the CPU spent handling cache misses due to lock operations. Due to the microarchitecture handling of locks, they are classified as L1 Bound regardless of what memory source satisfied them.</p><h4 id="gt-gt-gt-Split-Loads"><a href="#gt-gt-gt-Split-Loads" class="headerlink" title="&gt;&gt;&gt; Split Loads"></a>&gt;&gt;&gt; Split Loads</h4><p>Throughout the memory hierarchy, data moves at cache line granularity - 64 bytes per line. Although this is much larger than many common data types, such as integer, float, or double, unaligned values of these or other types may span two cache lines. Recent Intel architectures have significantly improved the performance of such ‘split loads’ by introducing split registers to handle these cases, but split loads can still be problematic, especially if many split loads in a row consume all available split registers.</p><h4 id="gt-gt-gt-4K-Aliasing"><a href="#gt-gt-gt-4K-Aliasing" class="headerlink" title="&gt;&gt;&gt; 4K Aliasing"></a>&gt;&gt;&gt; 4K Aliasing</h4><p>When an earlier (in program order) load issued after a later (in program order) store, a potential WAR (write-after-read) hazard exists. To detect such hazards, the memory order buffer (MOB) compares the low-order 12 bits of the load and store in every potential WAR hazard. If they match, the load is reissued, penalizing performance. However, as only 12 bits are compared, a WAR hazard may be detected falsely on loads and stores whose addresses are separated by a multiple of 4096 (2^12). This metric estimates the performance penalty of handling such falsely aliasing loads and stores.</p><h4 id="gt-gt-gt-FB-Full"><a href="#gt-gt-gt-FB-Full" class="headerlink" title="&gt;&gt;&gt; FB Full"></a>&gt;&gt;&gt; FB Full</h4><p>This metric does a rough estimation of how often L1D Fill Buffer unavailability limited additional L1D miss memory access requests to proceed. The higher the metric value, the deeper the memory hierarchy level the misses are satisfied from. Often it hints on approaching bandwidth limits (to L2 cache, L3 cache or external memory).</p><h3 id="gt-gt-L2-Bound"><a href="#gt-gt-L2-Bound" class="headerlink" title="&gt;&gt; L2 Bound"></a>&gt;&gt; L2 Bound</h3><p>This metric shows how often machine was stalled on L2 cache. Avoiding cache misses (L1 misses/L2 hits) will improve the latency and increase performance.</p><h3 id="gt-gt-L3-Bound"><a href="#gt-gt-L3-Bound" class="headerlink" title="&gt;&gt; L3 Bound"></a>&gt;&gt; L3 Bound</h3><p>This metric shows how often CPU was stalled on L3 cache, or contended with a sibling Core. Avoiding cache misses (L2 misses/L3 hits) improves the latency and increases performance.</p><h4 id="gt-gt-gt-Contested-Accesses"><a href="#gt-gt-gt-Contested-Accesses" class="headerlink" title="&gt;&gt;&gt; Contested Accesses"></a>&gt;&gt;&gt; Contested Accesses</h4><p>Contested accesses occur when data written by one thread is read by another thread on a different core. Examples of contested accesses include synchronizations such as locks, true data sharing such as modified locked variables, and false sharing. This metric is a ratio of cycles generated while the caching system was handling contested accesses to all cycles.</p><h4 id="gt-gt-gt-Data-Sharing"><a href="#gt-gt-gt-Data-Sharing" class="headerlink" title="&gt;&gt;&gt; Data Sharing"></a>&gt;&gt;&gt; Data Sharing</h4><p>Data shared by multiple threads (even just read shared) may cause increased access latency due to cache coherency. This metric measures the impact of that coherency. Excessive data sharing can drastically harm multithreaded performance. This metric is defined by the ratio of cycles while the caching system is handling shared data to all cycles. It does not measure waits due to contention on a variable, which is measured by the Locks and Waits analysis.</p><h4 id="gt-gt-gt-L3-Latency"><a href="#gt-gt-gt-L3-Latency" class="headerlink" title="&gt;&gt;&gt; L3 Latency"></a>&gt;&gt;&gt; L3 Latency</h4><p>This metric shows a fraction of cycles with demand load accesses that hit the L3 cache under unloaded scenarios (possibly L3 latency limited). Avoiding private cache misses (i.e. L2 misses/L3 hits) will improve the latency, reduce contention with sibling physical cores and increase performance. Note the value of this node may overlap with its siblings.</p><h4 id="gt-gt-gt-SQ-Full"><a href="#gt-gt-gt-SQ-Full" class="headerlink" title="&gt;&gt;&gt; SQ Full"></a>&gt;&gt;&gt; SQ Full</h4><p>This metric measures fraction of cycles where the Super Queue (SQ) was full taking into account all request-types and both hardware SMT threads. The Super Queue is used for requests to access the L2 cache or to go out to the Uncore.</p><h3 id="gt-gt-DRAM-Bound"><a href="#gt-gt-DRAM-Bound" class="headerlink" title="&gt;&gt; DRAM Bound"></a>&gt;&gt; DRAM Bound</h3><p>This metric shows how often CPU was stalled on the main memory (DRAM). Caching typically improves the latency and increases performance.</p><h4 id="gt-gt-gt-Memory-Bandwidth"><a href="#gt-gt-gt-Memory-Bandwidth" class="headerlink" title="&gt;&gt;&gt; Memory Bandwidth"></a>&gt;&gt;&gt; Memory Bandwidth</h4><p>This metric represents a fraction of cycles during which an application could be stalled due to approaching bandwidth limits of the main memory (DRAM). This metric does not aggregate requests from other threads/cores/sockets (see Uncore counters for that). Consider improving data locality in NUMA multi-socket systems.</p><h4 id="gt-gt-gt-Memory-Latency"><a href="#gt-gt-gt-Memory-Latency" class="headerlink" title="&gt;&gt;&gt; Memory Latency"></a>&gt;&gt;&gt; Memory Latency</h4><p>This metric represents a fraction of cycles during which an application could be stalled due to the latency of the main memory (DRAM). This metric does not aggregate requests from other threads/cores/sockets (see Uncore counters for that). Consider optimizing data layout or using Software Prefetches (through the compiler).</p><p><strong>LLC Miss: </strong>The LLC (last-level cache) is the last, and longest-latency, level in the memory hierarchy before main memory (DRAM). Any memory requests missing here must be serviced by local or remote DRAM, with significant latency. The LLC Miss metric shows a ratio of cycles with outstanding LLC misses to all cycles.</p><h3 id="gt-gt-Store-Bound"><a href="#gt-gt-Store-Bound" class="headerlink" title="&gt;&gt; Store Bound"></a>&gt;&gt; Store Bound</h3><p>This metric shows how often CPU was stalled on store operations. Even though memory store accesses do not typically stall out-of-order CPUs; there are few cases where stores can lead to actual stalls. Consider False Sharing analysis as your next step.</p><h4 id="gt-gt-gt-Store-Latency"><a href="#gt-gt-gt-Store-Latency" class="headerlink" title="&gt;&gt;&gt; Store Latency"></a>&gt;&gt;&gt; Store Latency</h4><p>This metric represents cycles fraction the CPU spent handling long-latency store misses (missing 2nd level cache). Consider avoiding/reducing unnecessary (or easily loadable/computable) memory store. Note that this metric value may be highlighted due to a Lock Latency issue.</p><h4 id="gt-gt-gt-False-Sharing"><a href="#gt-gt-gt-False-Sharing" class="headerlink" title="&gt;&gt;&gt; False Sharing"></a>&gt;&gt;&gt; False Sharing</h4><p>This metric shows how often CPU was stalled on store operations to a shared cache line. It can be easily avoided by padding to make threads access different lines.</p><h4 id="gt-gt-gt-Split-Stores"><a href="#gt-gt-gt-Split-Stores" class="headerlink" title="&gt;&gt;&gt; Split Stores"></a>&gt;&gt;&gt; Split Stores</h4><p>This metric represents a rate of split store accesses. Consider aligning your data to the 64-byte cache line granularity.</p><h4 id="gt-gt-gt-DTLB-Store-Overhead"><a href="#gt-gt-gt-DTLB-Store-Overhead" class="headerlink" title="&gt;&gt;&gt; DTLB Store Overhead"></a>&gt;&gt;&gt; DTLB Store Overhead</h4><p>This metric represents a fraction of cycles spent on handling first-level data TLB store misses. As with ordinary data caching, focus on improving data locality and reducing working-set size to reduce DTLB overhead. Additionally, consider using profile-guided optimization (PGO) to collocate frequently-used data on the same page. Try using larger page sizes for large amounts of frequently-used data.</p><h2 id="gt-Core-Bound"><a href="#gt-Core-Bound" class="headerlink" title="&gt; Core Bound"></a>&gt; Core Bound</h2><p>This metric represents how much Core non-memory issues were of a bottleneck. Shortage in hardware compute resources, or dependencies software’s instructions are both categorized under Core Bound. Hence it may indicate the machine ran out of an OOO resources, certain execution units are overloaded or dependencies in program’s data- or instruction- flow are limiting the performance (e.g. FP-chained long-latency arithmetic operations).</p><h3 id="gt-gt-Divider"><a href="#gt-gt-Divider" class="headerlink" title="&gt;&gt; Divider"></a>&gt;&gt; Divider</h3><p>Not all arithmetic operations take the same amount of time. Divides and square roots, both performed by the DIV unit, take considerably longer than integer or floating point addition, subtraction, or multiplication. This metric represents cycles fraction where the Divider unit was active.</p><h3 id="gt-gt-Port-Utilization"><a href="#gt-gt-Port-Utilization" class="headerlink" title="&gt;&gt; Port Utilization"></a>&gt;&gt; Port Utilization</h3><p>This metric represents a fraction of cycles during which an application was stalled due to Core non-divider-related issues. For example, heavy data-dependency between nearby instructions, or a sequence of instructions that overloads specific ports. Hint: Loop Vectorization - most compilers feature auto-Vectorization options today - reduces pressure on the execution ports as multiple elements are calculated with same uOp.</p><h1 id="Retiring"><a href="#Retiring" class="headerlink" title="Retiring"></a>Retiring</h1>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Economics in One Lesson]]></title>
      <url>http://hamstersi.github.io/2017/02/Economics-in-One-Lesson/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h3 id="关于这堂课"><a href="#关于这堂课" class="headerlink" title="关于这堂课"></a>关于这堂课</h3><p>经济学的艺术，在于不仅要观察任何行为或整车的即期影响，更要考察比较长远的影响；不仅要关注政策对某个群体产生的影响，更要追踪对所有群体造成的影响。</p><h3 id="课程的应用"><a href="#课程的应用" class="headerlink" title="课程的应用"></a>课程的应用</h3><h4 id="gt-破橱窗"><a href="#gt-破橱窗" class="headerlink" title="&gt; 破橱窗"></a>&gt; 破橱窗</h4><p>话说一个顽童抡起砖头，砸破了面包店的橱窗。当店主怒气冲冲追出来时，小捣蛋早已逃之夭夭，只剩下一群看闹热的围观者。大家盯着橱窗的破洞以及四下散落的玻璃碎片，有些人开始互相议论，宽慰店主：玻璃破了很是可惜，可是这也有好的一面。这不，对面的玻璃店又有生意了。他们越琢磨越来劲：一面新的橱窗需要250美元。玻璃店多了250美元，会去别的商家那里消费，那些个商家的口袋里多了几个钱，又会向更多的商家买东西。经这么一说，小小一片破橱窗，竟能够连环不断提供资金给很多商家，使很多人获得就业机会。要是照这个逻辑推下去，结论便是：扔砖头的那个小捣蛋，不但不是社区的祸害，反而是造福社区的善人。</p><a id="more"></a><p>但是，面包店主损失掉的250美元，原本是打算拿去做一套西装的。如今，这钱被迫挪去补破窗。他原来在准备去做西装的那个下午，他本来可以心满意足同时拥有橱窗和西装，结果却只能面对有了橱窗就没了西装的糟糕现实。简单来说，玻璃店主的这桩生意，不过是从做西装的缝纫店主那里转移来的。整个过程并没有新增“就业机会”。那些围观的人只想到了交易双方：面包店主和玻璃店主的情况，却忘掉了可能涉及的第三方缝纫店主的窘迫。</p><h4 id="gt-战祸之福"><a href="#gt-战祸之福" class="headerlink" title="&gt; 战祸之福"></a>&gt; 战祸之福</h4><p>有些人不屑于谈小小的破坏行为带来的蝇头小利，却醉心于巨大的破坏行为能让人们受益无穷。他们吹嘘战争对经济是如何如何的有利，非和平时期能比，并向我们展示通过战争才能实现的“生产奇迹”。他们认为，战争时期庞大的需求“累积”或“堵塞”，会给战后的世界带来繁荣。</p><p>这种“需求堵塞”谬论只不过是我们所熟悉的老朋友——破窗谬论——换上一件臃肿的马甲之后的形象而已。不过这一次，有更多相关的谬误绞缠在一起，需要我们逐一驳斥。首先，它把需要(need)和需求(demand)混为一谈。战火摧毁的东西越多，人民生活越贫困，战后需要的东西也就越多，这点毋庸置疑。但是，需要并不等于需求。有效的经济需求，光有需要还不算，还必须要有相当的购买力才行。当今印度对产品的实际需要远高于美国，但是它的购买力，以及由此带来的创造商机的能力却远低于美国。</p><p>人们只习惯于用金钱来衡量自己的财富和收入，所以只要手头多了几张钞票，便以为自己过得更好，尽管拿这些钱能买到的东西比从前少，自己实际拥有的东西可能不如从前。人们所认为的第二次世界大战带来的经济“收益”，其实大多是战时通货膨胀造成的幻象。哪怕在和平年代，同等规模的通货膨胀也能带来这样的结果，并且的确产生过这些结果。</p><p>那些认为战争造成的破坏能增加总体“需求”的人，还遗漏了一个基本事实：需求和供给就像硬币的两面，其实是从不同角度观察到的同一样东西。供给会创造需求，因为归根结底供给就是需求。人们把自己生产的东西供应给他人，其实是为了换得自己想要的东西。农民为城市供应小麦，是因为他们需要汽车或其他产品。所有这些，是现代分工和交换经济的本质。</p><h4 id="gt-公共工程来自缴税"><a href="#gt-公共工程来自缴税" class="headerlink" title="&gt; 公共工程来自缴税"></a>&gt; 公共工程来自缴税</h4><p>我们必须认识到：政府所有的支出最后都必须靠纳税人来埋单；通货膨胀本身只是税收的一种表现形式，并且是极其有害的形式。</p><p>一定数额的公共开支对执行基本的政府职能是必要的。一定数量的公共设施建设，如街道、桥梁、隧道、军营、海军基地、议会大厦、警察局和消防队，是提供基本的公共服务所必需的。社会对这些公共建设本身有需要，需要就是其存在的理由。但如果是为了“提高就业机会”而建桥，那就成另外一回事了。当提供就业机会成了目的之后，有无兴建桥梁的实际需要就会成为次要问题。这时，政府必须无中生有，发明各种“公共建设计划”。他们不再只考虑哪里必须建桥，而是开始自问自答：桥可以建在哪里。</p><p>不难发现，造桥工程不过是使就业机会发生了转移而已。造桥工程每制造一个用于公共建设上的工作机会，就必定会破坏掉一个用于私人领域的工作机会。我们看得见桥梁工地上建设者们夜以继日，但有些东西我们是看不到的。这些看不到的东西，是从纳税人的口袋拿走1000万美元之后而破坏掉的工作机会，一方面造桥工人在增加，另一方面汽车工人、电视机工人、制衣工人、农民在越少。</p><h4 id="gt-税负抑制生产"><a href="#gt-税负抑制生产" class="headerlink" title="&gt; 税负抑制生产"></a>&gt; 税负抑制生产</h4><p>我们都知道，在现代社会中，每个人所承受的所得税比例不尽相同。为了弥补公共开支，政府还必须开征其他名目繁多的赋税，从而进一步挫伤了生产者的积极性。没有人情愿自己的钱被政府拿走，税负不可避免地会影响纳税人的行为和动机。如果一家公司发生亏损，每赔一块钱，就得足足损失一块钱；要是没有适当的税务会计法规，允许拿数年来的亏损去冲抵数年来的利润的话，当这家公司赚钱的时候，每赚一块钱，却只能留下税后的部分(例如52美分)。于是，公司的经营政策就会受到影响，它将丧失扩张业务的冲动，或者只扩张那些风险最低的业务。觉察到这种状况的人甚至会打消开创新事业的念头。现有的雇主将不再增加员工，甚至会设法减员；其他人则根本不打算成为雇主。长期下来，与本来可以达到的水准相比，新设备和新工艺的应用放慢，消费者买不到更好更便宜的产品，实际工资不升反降。</p><h4 id="gt-政府信贷扭曲生产"><a href="#gt-政府信贷扭曲生产" class="headerlink" title="&gt; 政府信贷扭曲生产"></a>&gt; 政府信贷扭曲生产</h4><p>有时，政府拿钱出来“扶持”企业，这其实和政府征敛民财一样可怕。扶持的形式通常表现为政府直接贷款，以及政府担保的民间贷款等。</p><p>私人提供贷款与政府提供贷款这二者有根本的不同。每个私营放贷者都是在用自己的资金承担风险。银行家虽然是利用别人委托给他的钱去冒险，但一旦有损失，也必须拿自己的钱去赔付，否则只有破产出局。当人们拿自己的钱去冒险时，通常会严格审查借款人是否有足够的资产做抵押，审查其经营能力和诚信如何。</p><p>倘若政府依照同样严格的标准去办理贷款业务，那政府根本没有必要涉足这一行，去做私营机构已经在做的事。政府之所以涉足贷款业，就是要向那些从私营机构借不到钱的人提供资金。换句话说，私营放贷者不肯拿自己的钱去冒的风险，政府却愿意拿纳税人的钱去冒险。这种措施的支持者也承认政府放贷的坏帐率高于民间放贷，但他们坚持认为，那些有借有还的人所增加的产出，加上大部分有借无还的人所增加的产出，在抵消坏账损失后仍有剩余。</p><p>政府贷款的一种后果：浪费资本和削弱生产。政府信贷会把可用的资本丢进糟糕的计划中，充其量也是丢进好坏难断的计划中。政府信贷更可能把资本交给能力比较差、或者比较不可靠的人。可是在任何时候，实体资本的数量都是有限的(有别于靠印钞机印出来的货币资本)，交到某乙手中的东西，就不可能再交给某甲。</p><h4 id="gt-诅咒机器"><a href="#gt-诅咒机器" class="headerlink" title="&gt; 诅咒机器"></a>&gt; 诅咒机器</h4><p>在所有的经济错觉中，“机器必然导致失业”这种错误最为阴魂不散。这种错觉曾经被无数人驳倒过，但总能死灰复燃，并且和以往一样张狂。</p><p>如果采用省力机器确实会造成失业率不断上升、加剧不幸的话，我们将合乎逻辑地得出颠覆性的结论，不仅会颠覆技术领域的观念，而且会颠覆整个人类文明的观念。我们不仅应该把任何的新技术进步都视为一场灾难，而且更该觉得过去所有的技术进步也都同样恐怖。大大小小的雇主，总在设法通过节约劳动力来提高经济效益。头脑灵活的工人，都会想办法以最少的付出去完成上面指派的工作。雄心勃勃的人，总在坚持不懈地跟时间赛跑。如果严守逻辑上的一致性，那么恐惧科技进步的人必须摒弃所有这些进步和智巧，因为技术进步不但无益，而且有害。比方说从芝加哥运货到纽约，要是我们能够大量雇用人力，我们何必还要用火车，让人扛起货物背过去得了。</p><p>我们需要重申，机器所带来的是促进生产和提高生活水平。这个结果可以通过两条途径来实现：机器使消费者购买的产品变得更加便宜，或者提高工人的生产力，从而使工人的工资能够提高。换句话说，机器能够提高货币工资，或者能够降低物价，让同样的薪水能买到更多的产品和服务。</p><h4 id="gt-分散工作机会的企图"><a href="#gt-分散工作机会的企图" class="headerlink" title="&gt; 分散工作机会的企图"></a>&gt; 分散工作机会的企图</h4><p>“制造工作机会”和“限产超雇”的这些做法的起因以及公众容忍它们的起因，跟害怕机器的起因一样，都源自相同的基本谬论：用更有效率的方式去做事，只会消减工作机会。这个信条换句话说就是，采用低效率的方式去做一件事，反而可以创造工作机会。和这个谬论有关的另一种论调认为，世界上可做的工作是有限的，要是我们想不出更繁琐拖沓的做事方式来增加工作量，那么我们至少可以想方设法将事情分摊给尽可能多的人去做。</p><p>倘若大家各自为政，少数人的确可以从这种不合理的细部分工规定中获利，然而这种呆板的规定是以牺牲其他人的利益为代价的。支持这类做法的人没有看到生产成本一定会因此增高，从总体上看，最终结果是工作做得更少，生产出的产品更少。</p><h4 id="gt-遣散军队和裁减公务员"><a href="#gt-遣散军队和裁减公务员" class="headerlink" title="&gt; 遣散军队和裁减公务员"></a>&gt; 遣散军队和裁减公务员</h4><p>每逢战争结束，军人复员的时候，人们总是担忧没有足够多的工作岗位来安置这些复员军人，担心这些人会失业。但是，军人复员转业之后的经济，与遣散之前的经济不会相同。以前靠平民养活的军人，而今是自食其力的平民。战争结束了，国防建设也用不了那么多军人，继续把他们留在军中就是徒劳无益的事情，因为纳税人出钱供养他们是得不到相应回报的。现在，纳税人能把这一部分的钱支付给复员转业人员，换取等值的产品或服务。国民生产总值，以及每个人的财富，都会增加更多。同样的推理也适用于政府裁减冗员的情况。由于人浮于事，这些公务员为社会提供的服务配不上他们所获得的薪酬。</p><p>裁掉冗员之后的国家比留用冗员时，不会更糟，只会更好。因为这部分人必须自谋职业或自己创业。纳税人购买力增加，将为被裁公务员提供更多就业机会，这跟军人复员的情形一样。并且，这些人只有为雇主或顾客提供等值服务，才能在社会立足。他们不再是社会的寄生虫，而成为直接为社会贡献生产力的个人。</p><h4 id="gt-盲目崇拜充分就业"><a href="#gt-盲目崇拜充分就业" class="headerlink" title="&gt; 盲目崇拜充分就业"></a>&gt; 盲目崇拜充分就业</h4><p>与每个人的经济目标一样，任何国家的经济目标也应该是用最少的付出获得最大的收益。中国和印度远比美国贫穷，但它们的主要问题也不是失业问题，而是生产方式太落后(这既是资本短缺的原因，也是资本短缺的结果)。</p><p>文明的进步其实体现在就业人口的减少上，而不是体现在增加上。这是因为当国家变得日益富裕之后，我们得以禁止使用童工，得以实现老有所养。只要我们把重点放在该放的地方，也就是采取生产最大化的政策，我们就不会迷失。</p><h4 id="gt-关税“保护”了哪些人？"><a href="#gt-关税“保护”了哪些人？" class="headerlink" title="&gt; 关税“保护”了哪些人？"></a>&gt; 关税“保护”了哪些人？</h4><p>关税常被看作是以牺牲消费者来造福生产者的一种手段。从某种意义上讲，这话大致不错。那些主张征收关税的人，只考虑到受关税保护的生产者能立即获得利益，却忽略被迫支付关税的消费者会立即遭受损失。但把关税只看成是生产者利益与消费者利益之间的冲突，却是不对的。关税的确让所有的消费者蒙受损失，但并非所有的生产者都从中获益。前面谈过，关税只对受保护的生产者有帮助，为此不惜牺牲其他所有的美国生产者：尤其是对那些出口潜力相对较大的生产者利益的损害。</p><h4 id="gt-拼命出口"><a href="#gt-拼命出口" class="headerlink" title="&gt; 拼命出口"></a>&gt; 拼命出口</h4><p>长期而言，进口与出口必然相等(这里的进出口指“国际收支账户”里的所有项目，包括“无形”项目，如旅游消费、海运费用等)。有出口，才有钱进口；没有进口，就没有机会出口，因为外国人没有美元可以用来买美国的产品。要想扩大出口，就必须有更多的进口，否则收不到货款。要想压缩进口，出口也会随之被压缩。</p><h4 id="gt-“平准价格”"><a href="#gt-“平准价格”" class="headerlink" title="&gt; “平准价格”"></a>&gt; “平准价格”</h4><p>平准价格的说法大致如下。在所有产业中，农业最基本、最重要，必须不惜一切代价加以保护。还说，只有农民富裕了，其他人的富裕才有着落。要是农民缺乏购买力，买不起工业产品，工商业就会萎缩。</p><p>政府干预并没有停留在造成购买力的转移，也就是把购买力从城市消费者(或一般纳税人，或两者兼而有之)，转移到农民手上。政府干预往往通过限制农产品的生产以抬高价格，这样会造成财富灭失，因为可供消费农产品被人为减少了。</p><h4 id="gt-救救X产业"><a href="#gt-救救X产业" class="headerlink" title="&gt; 救救X产业"></a>&gt; 救救X产业</h4><p>拯救X产业的办法不胜枚举。相关提案大抵有两大类。其中一类强调，X产业已经“过度拥挤”，政府应该阻止其他公司或劳工再进入。另一类则呼吁，政府应该以直接补贴的方式对X产业给予扶持。</p><p>如果X产业与其他产业相比确实过度拥挤，那根本不需要任何强制立法，去排斥新资本或新劳工进入。新资本不会抢着挤进显然要垮掉的行业。投资人不会冒然踏进风险最大、回报率最低的行业。劳工如果能更好的选择，同样不会进入工资最低、工作最不稳定的行业。如果新资本和新劳工是被强制排斥在X产业之外，例如垄断、同业联盟、工会的政策或者法律等强制手段剥夺了这些资源的自由选择。投资人只好将钱投向回报率还不如X产业的其他地方。其结果是产量减少，进而导致生活水平普遍下降。</p><p>从国库直接拿钱来补贴X产业，这只不过是将财富或收益转移到X产业。X产业中的人获得多少，纳税人就损失多少。但是，补贴的结果不仅仅是财富或收益的转移，也不仅仅意味其他产业的总体萎缩程度和X产业的扩张程度相当。</p><h4 id="gt-价格体系如何运作？"><a href="#gt-价格体系如何运作？" class="headerlink" title="&gt; 价格体系如何运作？"></a>&gt; 价格体系如何运作？</h4><p>在现代这样一个错综复杂的社会中，人们的需要和要求及其轻重缓急千差万别，如何解决劳力和资本用在哪里的问题？解决之道正是通过价格体系，也就是通过生产成本、价格和利润三者间不断变动的交互关系来调节的。</p><p>生产成本决定价格这样的理论是不对的。价格取决于供给和需求，而需求取决于人们想要拥有某种商品渴求程度，以及由人们用来交换的东西所决定。认为供给有一部分取决于生产成本是对的。但一种商品已经发生的生产成本却不能决定商品的价值。商品价值取决于现在发生的供需关系。对于一种商品未来生产成本和市场价位的预期，将会决定<br>那种商品未来的生产数量，这种预期就会影响未来的供给。因此，一种商品的价格与其边际生产成本总是趋于彼此相等，但并不是由于边际生产成本直接决定价格。</p><p>经济处于均衡状态时，某个行业的扩张，必然要以其他行业牺牲为代价。生产要素在任何时候都是有限的。只有当劳工、土地和资本，从原来的行业转移到了某个行业，那个行业才能够扩张。当某一行业萎缩或者停止增产，并不必然意味着总体生产出现净减少。因为该行业萎缩所释放出劳工和资本，可供其他的行业用于扩张。简单地说，生产每一样东西，都得以牺牲放弃其他某样东西为代价。</p><h4 id="gt-“稳定”物价"><a href="#gt-“稳定”物价" class="headerlink" title="&gt; “稳定”物价"></a>&gt; “稳定”物价</h4><p>在自由竞争的市场经济中，由于价格下跌而被逐出市场的，都是高成本、低效率的生产者。政府一刀切限制生产的结果，意味着效率高、成本低的生产者不准以低价供应其全部产量，同时意味着效率低、成本高的生产者仍将被人为地保留在业内。这就使得产品的平均生产成本提高，生产效率下降。因为土地、人力和资本这些资源被无效率的边际生产者继续占用，就无法转移到收益和效率更高的地方。</p><p>政府规划官员在这里所说的自由贸易到底是什么意思，我弄不清楚，但是他们的言下之意不包含哪些事情，我们倒是相当确定。他们的意思绝对不是说，平民百姓可以自由地从事买卖或者借贷，自由谈判价格或利率，自由出入他们觉得收益最大的地方。他们的意思绝对不是说，平民百姓可以自主决定产量、可以自由移民、可以自由支配自己的资本和财物。我怀疑他们的意思是说，由官僚来替平民百姓决定这些事情，并且告诉平民，服从计划奖赏将是生活水平的提高。</p><h4 id="gt-政府管制价格"><a href="#gt-政府管制价格" class="headerlink" title="&gt; 政府管制价格"></a>&gt; 政府管制价格</h4><p>当商品价格被人为限制在它的市场价位之下时，不可避免地会带来两个结果。第一是导致受控商品的需求增加。由于该商品变得便宜，图便宜的人会更多，人们也买得起更多。第二是导致受控商品供给减少。由于人们买得更多，该商品势必供不应求。但是生产积极性却遭到打击，降价致使该商品收益率降低，甚至做不出利润。边际生产者被迫出局。即使最有效率的生产者也可能亏本经营。</p><p>价格上涨的真正原因，是商品匮乏或货币过剩。法定价格上限根本无法解决这两方面的问题。事实上，这只会加剧商品短缺。人们当然不想花更多的钱去购买牛奶、黄油、鞋子、家具、戏票、钻石，去支付房租。对于每个人来说，惟一该涨价的，是他自己生产的东西；这是一个人理解并赞成价格上涨的原因。不过，人人都觉得自己的产品才应该最应该涨价。</p><p>我们每个人都具备多重经济角色。每个人都是生产者、纳税人，消费者。作为生产者时，他希望涨价(因为顾及自己的服务或产品)；作为消费者时，他希望降价(因为顾及自己的钱口袋)。作为消费者，他可能主张或者默许政府实施补贴；作为纳税人，他反对政府实施补贴。每个人都心照不宣想从各种政治力量的斗争中获利，例如让自己产品涨价而不让原材料涨价，或者作为消费者支持价格管制。</p><h4 id="gt-房租管制的后果"><a href="#gt-房租管制的后果" class="headerlink" title="&gt; 房租管制的后果"></a>&gt; 房租管制的后果</h4><p>不合理的价格管制施加在面包等日用消费品上的时候，面包店只要停止烘焙、停止出售面包就行。面包马上就会供不应求，迫使政治人物提高价格上限，甚至取消价格管制。房租管制则有所不同。一方面，房东除了继续把房子租出去，通常没有别的选择，因为在扣除税金和抵押贷款利息之后总还有点净收入，尽管无力维修。另一方面，由于房屋经久耐用，可能要住上好几年，租户才会开始感受到房东无力维修的苦果。也许再过数年，人们才会意识到房荒与房屋设施恶化，与房租管制有直接的关系。在普通价格管制取消多年之后，房租管制仍在继续，因为政治人物知道租户的票数比房东的票数更多。</p><p>房租管制迟迟不能取消，其压力来自于只顾某个群体短期利益的那些人。如果我们考虑每个人(包括租户本身)受到的长期影响，我们就会认识到房租管制徒劳无益，就会认识到管制手段越严厉、实施时间越长，它的破坏性越大。</p><h4 id="gt-最低工资法"><a href="#gt-最低工资法" class="headerlink" title="&gt; 最低工资法"></a>&gt; 最低工资法</h4><p>我们已经看到政府以行政干预提高某些商品的价格所带来的破坏作用。通过法律规定最低工资标准来提高工资，也会造成类似的破坏。这不足为奇，因为工资其实就是劳工提供服务的价格。很不幸，这种价格竟然取了个与其他的价格迥然不同的名称，这会让人思维不清楚。</p><p>最低工资法的出发点是要消灭低工资，但事实表明，这样的法律不但在保护劳工方面效果有限，而且弊大于利的程度和其目标成正比。提高工资的最佳手段，是提高边际劳工的生产力。这可以通过许多方法来实现：通过增加资本累积，例如添置机器以协助劳工；通过引进新的发明和改进；通过提高管理效率；通过激励更加勤奋和更有效劳作；通过更好的教育培训等。</p><h4 id="gt-工会真的有办法提高工资吗？"><a href="#gt-工会真的有办法提高工资吗？" class="headerlink" title="&gt; 工会真的有办法提高工资吗？"></a>&gt; 工会真的有办法提高工资吗？</h4><p>工会会员不是慈善家和圣人，靠工资吃饭的劳工也不是。我们都渴望获得经济公平，但这种所谓的公平其实是，渴望自己拥有东西与比自己过得好的人一样多，而不希望比自己过得差的人拥有与自己一样多的东西。不过，我们现在不去关心这种人性上的劣根性，我们现在所关心的，是特定经济理论是否合乎逻辑和合理。</p><h4 id="gt-“有足够的钱买回商品”"><a href="#gt-“有足够的钱买回商品”" class="headerlink" title="&gt; “有足够的钱买回商品”"></a>&gt; “有足够的钱买回商品”</h4><p>经济均衡(equilibrium)的一般意义和影响的问题上。均衡工资和价格是指使供给和需求均衡时的工资和价格。如果通过政府或私人强行将价格提升到高于它们的均衡水平，需求将下降，从而导致生产萎缩。如果强行将价格压到低于它们的均衡水平，随之而来的利润的下降将意味着供应下降或生产缩减。所以任何迫使价格高于或低于均衡水平(自由市场经常使之趋于这个水平)的企图，都将导致就业量和生产量缩减到低于它们应有的水准。</p><p>价格、工资和利润，能决定产品分配。最好的价格并非最高价格，而是能促进达到最大产量和最大销售量的价格。最好的工资率也不是最高的工资率，而是能达到充分生产、充分就业和持久稳定的最大总工资的工资率。不论从业界还是从劳工的角度来看，最好的利润不是指最低的利润，而是能够鼓励更多人投资、创业，更多人成为雇主，能提供更多就业机会的利润。</p><h4 id="gt-利润的功能"><a href="#gt-利润的功能" class="headerlink" title="&gt; 利润的功能"></a>&gt; 利润的功能</h4><p>利润的一大功能是引导生产要素的流向，依照需求来配置各种商品的相对产出。利润的实现不是靠提高价格，而是靠降低生产成本，靠讲求经济效益和效率。利润来自成本和价格之间的关系，它不仅告诉我们生产何种产品最经济，而且告诉我们哪种生产方式最经济。</p><h4 id="gt-通货膨胀的幻景"><a href="#gt-通货膨胀的幻景" class="headerlink" title="&gt; 通货膨胀的幻景"></a>&gt; 通货膨胀的幻景</h4><p>通货膨胀一旦启动便不受控制。我们不可能引导其平缓温和地结束，我们无法扭转随之而来的经济萧条。甚至不可能在预先设定的时点，或在价格上涨到预先设定的水平而让通货膨胀暂停。靠政治力量和经济力量都无法驾驭通货膨胀。那些能从通货膨胀中获得利益的政治集团，必然坚持继续维持通货膨胀。</p><p>通货膨胀的真正作用是改变价格和成本之间的关系。它所带来的最大改变，是提高相对于工资率的商品价格，借此来恢复价格和生产成本之间可以持续经营的关系，进而能借恢复经营利润来盘活闲置资源，鼓励恢复生产。</p><p>通货膨胀给每一经济过程罩上了一层迷惑人的面纱，欺骗了几乎所有人，包括那些受其害的人。当我们听到，国民收入(以货币计量)比通货膨胀之前增加一倍，谁不会觉得更富有更骄傲呢？原先周薪75美元的小职员，现在能拿120美元，他也会觉得比以前更强。他当然能感觉到生活费用涨得厉害，但是他并没有充分觉察到自己的真实处境。虽然同样是实质购买力降低，他无法接受生活费用不变而工资调减的事实，但是他会欣然接受工资调增而生活费用上涨的事实。通货膨胀有如自我暗示、催眠术、麻醉剂，可以减轻手术时的痛苦。通货膨胀也像鸦片，吸上一口，就觉得自己无所不能。</p><h4 id="gt-抨击储蓄"><a href="#gt-抨击储蓄" class="headerlink" title="&gt; 抨击储蓄"></a>&gt; 抨击储蓄</h4><p>有些被认为是当今杰出经济学家的人，为了避免国家经济萧条，竟然抨击储蓄，鼓吹大肆消费。当有人对这些政策的长期效应提出质疑时，他们却像败家子讥讽父母的忠告那样不屑一顾地说：“何必看得那么远呢？要知道从长远来看，我们都是要死的。”这么一句轻率的戏言，却被人当作至理名言和大智慧，奉为经济生活中的信条。不幸的是，长远并不真的那么远，或远或近的过去实施的政策所带来的长期影响，已经给我们带来煎熬了。坏的经济学家昨天要我们置之不理的明天，转眼就成了今天。总之，现代世界中的“储蓄”，只是支出的另一种形式。两者的差别，通常在于前者把钱交给别人用于扩大生产。</p>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[2013 - 11 浮生奥克兰]]></title>
      <url>http://hamstersi.github.io/2017/02/2013-11-%E6%B5%AE%E7%94%9F%E5%A5%A5%E5%85%8B%E5%85%B0/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p>和家里和朋友告了别，我是听着这首歌踏入中土世界的。飞机盘旋在奥克兰上空的时候，心里特别感动。大概是被这云层下的异乡与自以为是的勇气给触动了。其实在奥克兰待得真的不算长，不久便迫于生计辗转各个城市，最终在基督城安顿了半年。</p><p>对于基督城的感情，那大概是习惯使然。而对于新西兰来说，最有标志性的还是要数奥克兰和惠灵顿。在码头听海鸥和诗人游唱，在山上看日落看霓虹渐起，望天空塔向跨年烟火许愿，我对陌生人说新年要快乐。那个时候我不曾相问未来，朋友们萍水相逢来来去去换了一批又一批，终究却还是独自上路。</p><a id="more"></a><p>我一直特别想念这中土世界，从未断过的想念，一想到她呀，心里就悸动。那个时候少年轻狂，如今也只是活了个不明不白。让我们再见吧，奥克兰、再见吧，新西兰——</p><div id="aplayer0" class="aplayer" style="margin-bottom:20px;width:99%"></div><script>new APlayer({element:document.getElementById("aplayer0"),narrow:!1,autoplay:!1,showlrc:0,music:{title:"Down by the Salley Gardens",author:"Joanie Madden",url:"http://link.hhtjim.com/xiami/1354490.mp3",pic:"http://wx4.sinaimg.cn/large/672d88aaly9fd34xh7c7kj20ti0ti1kx.jpg"}})</script><p><img src="http://wx2.sinaimg.cn/large/672d88aagy1fcutfciukmj23b4274b29.jpg" alt="Auckland | Ferry Terminal"><br><img src="http://wx3.sinaimg.cn/large/672d88aagy1fcutf5g35wj23b4274hdt.jpg" alt="Auckland | Ferry Terminal"><br><img src="http://wx3.sinaimg.cn/large/672d88aagy1fcuteo3qznj23b4274hbf.jpg" alt="Auckland | Mt. Eden"><br><img src="http://wx4.sinaimg.cn/large/672d88aagy1fcutey4l2vj23b4274hc8.jpg" alt="Auckland | Mt. Eden"><br><img src="http://wx1.sinaimg.cn/large/672d88aagy1fcutfgvtxmj23b42741kx.jpg" alt="Auckland | Sky Tower"><br><img src="http://wx4.sinaimg.cn/large/672d88aagy1fcutfo7pb1j23b4274e81.jpg" alt="Auckland | Sky Tower"></p>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Porting the Essential Dynamics/Molecular Dynamics method for large-scale nucleic acid simulations to ARCHER]]></title>
      <url>http://hamstersi.github.io/2017/02/Porting-the-Essential-Dynamics-Molecular-Dynamics-method-for-large-scale-nucleic-acid-simulations-to-ARCHER/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p>The Molecular Dynamics (MD) simulation is an important computer modelling and simulation technique to study the physical behaviours of molecules and atoms. It has been widely applied in various scientific fields, and is of great value to do the MD simulation on biomacromolecules such as proteins and DNA, rendering scientists and researchers a rich source of information about the biological macromolecules.</p><p>The goal of this project was to develop a parallel code to perform the MD simulation on the large-scale nucleic acid DNA systems with a newly proposed method, namely the Essential Dynamics (ED), after reviewing two current existing code versions.<a id="more"></a> The two codes are written in Fortran 95 with MPI and Python respectively. Both codes have implemented the new Essential Dynamics/Molecular Dynamics (ED/MD) simulation for the DNA systems but in a quite primitive mode that contain serious limitations either on the execution flexibility (the Fortran 95/MPI code) or on the performance (the serial Python code). The new code is developed in C++ with MPI, which is a more flexible parallel code version compared to the Fortran 95/MPI code in the aspect of the degree of parallelism, while the performance is significantly improved by contrast with the serial Python code. The new code is compiled and run on ARCHER during the development, but it also has fairly good portability that can be executed on other parallel platforms or in personal laptops.</p><p>The simulation results of the new C++/MPI code are verified according to the results of the Fortran 95/MPI code, and the performance is also analysed afterwards. Due to the new parallelisation strategy that is different from the Fortran 95/MPI code, the improved flexibility of the parallelism is at the expense of some message passing overheads. To reduce the overheads and improve the performance, several parallel schemes have been proposed and implemented. The performance now is quite reasonable and acceptable with less time wasted on the communication. However, the future development work can still be carried on to further improve the execution efficiency of the parallel code.</p>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Hello World]]></title>
      <url>http://hamstersi.github.io/2017/02/Hello-World/</url>
      <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><center><br>Hello World! This planet turns so fast, everything burns, ashes to ash, but for now you are mine…<br></center><a id="more"></a>]]></content>
    </entry>
    
  
  
</search>
